#!/usr/bin/env python3
"""
Phase 1 Model Download and Test
Downloads all required models directly and verifies they work
"""

print("="*80)
print("Phase 1 Model Setup - Direct Download")
print("="*80)

# Check dependencies
print("\nChecking dependencies...")
missing = []
try:
    import sentence_transformers
    print("‚úÖ sentence-transformers installed")
except ImportError:
    missing.append("sentence-transformers")
    print("‚ùå sentence-transformers NOT installed")

try:
    import transformers
    print("‚úÖ transformers installed")
except ImportError:
    missing.append("transformers")
    print("‚ùå transformers NOT installed")

try:
    import torch
    print("‚úÖ torch installed")
except ImportError:
    missing.append("torch")
    print("‚ùå torch NOT installed")

if missing:
    print(f"\n‚ö†Ô∏è  Install missing packages:")
    print(f"  pip install {' '.join(missing)}")
    exit(1)

# Model 1: Embeddings
print("\n" + "="*80)
print("Model 1/3: all-MiniLM-L6-v2 (Embeddings)")
print("="*80)
print("Purpose: Pattern similarity search")
print("Size: ~80MB")
print("Downloading...")

try:
    from sentence_transformers import SentenceTransformer
    embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    
    # Test
    test_patterns = [
        "Light turns on at 7:15 AM on weekdays",
        "Thermostat set to 72¬∞F at 6 AM",
        "Lock front door at 11 PM"
    ]
    embeddings = embed_model.encode(test_patterns)
    
    print("‚úÖ Model 1 downloaded and tested successfully!")
    print(f"   Embedding dimension: {embeddings.shape[1]}")
    print(f"   Test patterns encoded: {len(embeddings)}")
    
except Exception as e:
    print(f"‚ùå Error with Model 1: {e}")
    exit(1)

# Model 2: Re-ranker
print("\n" + "="*80)
print("Model 2/3: bge-reranker-base (Re-ranker)")
print("="*80)
print("Purpose: Re-rank top 100 ‚Üí best 10 patterns")
print("Size: ~1.1GB (full) or 280MB (INT8)")
print("Downloading standard version...")

try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    
    rerank_tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-reranker-base")
    rerank_model = AutoModelForSequenceClassification.from_pretrained("BAAI/bge-reranker-base")
    
    # Test
    test_pair = "query text [SEP] document text"
    inputs = rerank_tokenizer(test_pair, return_tensors='pt', truncation=True)
    outputs = rerank_model(**inputs)
    score = outputs.logits[0][0].item()
    
    print("‚úÖ Model 2 downloaded and tested successfully!")
    print(f"   Test re-ranking score: {score:.3f}")
    print(f"   Note: For production, use INT8 version: OpenVINO/bge-reranker-base-int8-ov")
    
except Exception as e:
    print(f"‚ùå Error with Model 2: {e}")
    print("   Continuing anyway (re-ranker is optional enhancement)...")

# Model 3: Classifier
print("\n" + "="*80)
print("Model 3/3: flan-t5-small (Classifier)")
print("="*80)
print("Purpose: Categorize patterns (energy/comfort/security/convenience)")
print("Size: ~300MB")
print("Downloading...")

try:
    from transformers import T5Tokenizer, T5ForConditionalGeneration
    
    t5_tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")
    t5_model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small")
    
    # Test
    test_prompt = """Classify as one word: energy, comfort, security, or convenience

Pattern: Turn on lights at 7:15 AM on weekdays

Category:"""
    
    inputs = t5_tokenizer(test_prompt, return_tensors='pt')
    outputs = t5_model.generate(**inputs, max_new_tokens=5)
    result = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print("‚úÖ Model 3 downloaded and tested successfully!")
    print(f"   Test classification: {result}")
    
except Exception as e:
    print(f"‚ùå Error with Model 3: {e}")
    exit(1)

# Final Summary
print("\n" + "="*80)
print("‚úÖ ALL MODELS DOWNLOADED AND TESTED!")
print("="*80)
print(f"\nüìä Model Stack Summary:")
print(f"   Total Size: ~1.5GB (standard models)")
print(f"   Location: ~/.cache/huggingface/hub/")
print(f"   Status: Ready for Phase 1 development")
print(f"\nüöÄ Next Steps:")
print(f"   1. (Optional) Convert to OpenVINO INT8 (reduces to 380MB)")
print(f"      - See: implementation/OPENVINO_SETUP_GUIDE.md")
print(f"   2. Start preprocessing pipeline development")
print(f"      - See: implementation/PHASE_1_QUICK_REFERENCE.md")
print(f"   3. Begin Week 1 tasks")
print(f"\nüí° Tip: You can use standard models now, optimize to INT8 later")
print(f"   Standard: 1.5GB, 650ms (works great)")
print(f"   INT8: 380MB, 230ms (production optimized)")
print("\n‚úÖ Ready to start Phase 1! üéâ\n")

