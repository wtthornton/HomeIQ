# OpenAI Token Limits and Cost Analysis

**Date:** November 19, 2025  
**Issue:** OpenAI rate limit exceeded (429 error)  
**Current Model:** GPT-5.1  
**Rate Limit:** 30,000 TPM (Tokens Per Minute) - **UPDATED: Tier 1 is now 500K TPM**  
**Exceeded Request:** 34,738 tokens

**‚ö†Ô∏è IMPORTANT UPDATE:** OpenAI significantly increased rate limits in September 2025. If you're seeing 30K TPM limits, you may be on a legacy tier or need to verify your account tier.

---

## Problem Summary

The Ask AI service is hitting OpenAI rate limits due to large input prompts caused by comprehensive entity enrichment features added in recent updates.

### Error Details
```
Error code: 429 - Request too large for gpt-5.1 in organization
Limit: 30,000 tokens per minute (TPM)
Requested: 34,738 tokens
```

### Root Cause
The `generate_suggestions_from_query` function (lines 2947-3600+ in `ask_ai_router.py`) builds comprehensive context including:
1. **Location-aware entity expansion** - Lines 3205-3335
2. **Comprehensive entity enrichment** - Lines 3127-3202
3. **Enrichment context** (weather, carbon, energy, air quality) - Lines 3131-3191
4. **Entity capabilities and attributes** - Full state data for all entities
5. **Conversation history** - Clarification Q&A context

---

## Current Token Usage Analysis

### Input Token Breakdown (Estimated)

Based on code analysis, a single Ask AI query can include:

| Component | Estimated Tokens | Notes |
|-----------|-----------------|-------|
| System Prompt | 500-800 | Base instructions for automation generation |
| User Query | 50-200 | User's natural language request |
| Entity Context JSON | 15,000-25,000 | **LARGEST COMPONENT** - All enriched entities with:<br>- Capabilities<br>- Attributes<br>- Historical patterns<br>- Device intelligence data |
| Enrichment Context | 2,000-5,000 | Weather, carbon, energy, air quality data |
| Clarification Q&A | 500-2,000 | Previous clarification exchanges |
| Device Intelligence | 1,000-3,000 | Device-specific metadata |
| **Total Input** | **19,050-36,000** | **Exceeds 30K limit at high end** |

### Output Token Limits (max_tokens settings)

| Endpoint | max_tokens | Purpose |
|----------|-----------|---------|
| Suggestion Generation | 1,200 | JSON automation suggestions |
| YAML Generation | 2,000 | Full automation YAML |
| Clarification Questions | 400 | Question generation |
| Category Inference | 100 | Classification only |
| Entity Extraction | 300 | Entity identification |

---

## Cost Analysis: GPT-5.1 Pricing

### OpenAI GPT-5.1 Pricing (November 2025) ‚úÖ VERIFIED
- **Input tokens:** $1.25 per 1M tokens (50% cheaper than initial estimate)
- **Cached input tokens:** $0.125 per 1M tokens (90% discount)
- **Output tokens:** $10.00 per 1M tokens

### Cost Per Request (Current) ‚úÖ CORRECTED

**Average Ask AI Request:**
- Input: ~25,000 tokens = $0.03125 (was $0.0625 - 50% cheaper!)
- Output: ~800 tokens = $0.008
- **Total per request: $0.03925** (was $0.0705)

**YAML Generation (follow-up):**
- Input: ~20,000 tokens = $0.025 (was $0.0500)
- Output: ~800 tokens = $0.008
- **Total per YAML: $0.033** (was $0.0580)

**Complete Ask AI Flow (query ‚Üí suggestion ‚Üí YAML):**
- **Total: $0.07225 per automation created** (was $0.1285 - 44% cheaper!)

### Monthly Cost Estimates ‚úÖ CORRECTED

| Usage Scenario | Requests/Month | Cost/Month (Old) | Cost/Month (New) | Savings |
|---------------|----------------|------------------|------------------|---------|
| **Light** (10/day) | 300 | $38.55 | $21.48 | -44% |
| **Moderate** (30/day) | 900 | $115.65 | $64.43 | -44% |
| **Heavy** (100/day) | 3,000 | $385.50 | $214.75 | -44% |

### Annual Cost Projection ‚úÖ CORRECTED
- **Current usage (moderate):** $773.16/year (was $1,387.80 - 44% cheaper!)
- **Heavy usage:** $2,577.00/year (was $4,626.00 - 44% cheaper!)

---

## Recommendations

### Option 1: Optimize Current Architecture (RECOMMENDED)
**Goal:** Reduce input tokens by 40-60% without sacrificing quality

#### Changes:
1. **Enable Entity Filtering by Default** ‚úÖ
   - Already implemented (lines 3340-3456)
   - Filter entities by location + device name
   - **Savings:** 10,000-15,000 tokens per request
   - **Impact:** Minimal - only relevant entities included

2. **Make Enrichment Context Selective** ‚úÖ
   - Already feature-flagged: `ENABLE_ENRICHMENT_CONTEXT=true/false`
   - Disable weather/carbon/energy unless query-relevant
   - **Savings:** 2,000-4,000 tokens
   - **Impact:** None for most queries

3. **Limit Entity Attributes in Context**
   - Include only relevant attributes (not full state dumps)
   - Summarize capabilities instead of listing all
   - **Savings:** 3,000-5,000 tokens
   - **Impact:** Low - most attributes unused

4. **Implement Token Budget per Component**
   ```python
   TOKEN_BUDGET = {
       'entity_context': 10_000,      # Down from ~20,000
       'enrichment_context': 2_000,   # Down from ~5,000
       'system_prompt': 500,          # Fixed
       'conversation_history': 1_000, # Limit to last 2-3 turns
       'device_intelligence': 1_500,  # Summarize, don't dump
   }
   # Total: ~15,000 tokens (well under 30K limit)
   ```

5. **Add Token Counting and Logging**
   ```python
   import tiktoken
   
   def count_tokens(text: str, model: str = "gpt-5.1") -> int:
       encoding = tiktoken.encoding_for_model(model)
       return len(encoding.encode(text))
   ```

**Estimated Savings:** 15,000-20,000 tokens per request  
**New Input Range:** 10,000-16,000 tokens (well under 30K)  
**Cost Savings:** ~40% reduction ($231/year at moderate usage)

---

### Option 2: Verify/Upgrade OpenAI Tier (QUICK FIX) ‚ö†Ô∏è UPDATED
**Goal:** Verify current tier and increase rate limits if needed

#### Updated Tier Limits (September 2025) ‚úÖ VERIFIED:
- **Tier 1:** 500,000 TPM (was 30K - 16x increase!)
- **Tier 2:** 1,000,000 TPM (was 150K)
- **Tier 3:** 2,000,000 TPM
- **Tier 4:** 4,000,000 TPM

**If you're seeing 30K TPM limits:**
- You may be on a legacy tier or free tier
- Check your OpenAI account dashboard: https://platform.openai.com/account/rate-limits
- Tier upgrades may be automatic or require account verification
- **No additional cost** - limits increased for existing tiers

**Pros:**
- No code changes needed
- Immediate fix (if tier upgrade needed)
- Handles future growth
- **May already be available** - check your account

**Cons:**
- If already on Tier 1, you should have 500K TPM (not 30K)
- If still seeing 30K, may need account verification
- Doesn't address inefficiency (but less urgent now)

---

### Option 3: Switch to GPT-5 Mini/Nano (COST OPTIMIZATION) ‚úÖ UPDATED
**Goal:** Reduce costs dramatically for non-critical paths

#### GPT-5 Mini Pricing (November 2025) ‚úÖ VERIFIED:
- **Input:** $0.25 per 1M tokens (80% cheaper than GPT-5.1)
- **Cached Input:** $0.025 per 1M tokens (98% cheaper)
- **Output:** $2.00 per 1M tokens (80% cheaper)
- **Context window:** 400K tokens (same as GPT-5.1)

#### GPT-5 Nano Pricing (November 2025) ‚úÖ VERIFIED:
- **Input:** $0.05 per 1M tokens (96% cheaper than GPT-5.1)
- **Cached Input:** $0.005 per 1M tokens (99.6% cheaper)
- **Output:** $0.40 per 1M tokens (96% cheaper)
- **Context window:** 400K tokens

#### Cost Comparison ‚úÖ CORRECTED:

| Path | GPT-5.1 | GPT-5 Mini | GPT-5 Nano | Best Choice |
|------|---------|------------|------------|-------------|
| Suggestion Generation | $0.03925 | $0.01025 | $0.00245 | GPT-5.1 (creativity) |
| YAML Generation | $0.033 | $0.009 | $0.002 | GPT-5 Mini (balance) |
| **Monthly (900 reqs)** | **$64.43** | **$17.33** | **$4.41** | Hybrid |
| **Annual** | **$773.16** | **$207.96** | **$52.92** | Hybrid |

**Hybrid Strategy (Recommended):**
- GPT-5.1: Creative suggestion generation (30% of requests)
- GPT-5 Mini: YAML generation, entity extraction (60% of requests)
- GPT-5 Nano: Classification, simple tasks (10% of requests)
- **Estimated Hybrid Cost:** ~$25/month ($300/year) - 61% savings

#### Quality Assessment:
- **GPT-5.1:** Best reasoning, most creative suggestions, highest quality
- **GPT-5 Mini:** Good balance, sufficient for most tasks, 80% cheaper
- **GPT-5 Nano:** Fastest, cheapest, good for simple classification/extraction
- **Recommendation:** Hybrid approach with all three models

#### Hybrid Model Strategy ‚úÖ UPDATED:
```python
# High-value paths: Use GPT-5.1 ($1.25/$10 per 1M)
- Ask AI suggestion generation (creative ideas)
- Complex multi-step automations
- User-facing conversational AI

# Standard paths: Use GPT-5 Mini ($0.25/$2 per 1M)
- YAML generation (templated output)
- Entity extraction (pattern matching)
- Clarification question generation

# Simple paths: Use GPT-5 Nano ($0.05/$0.40 per 1M)
- Category inference (classification)
- Simple entity validation
- Basic text processing
```

**Estimated Hybrid Cost:** ~$25/month ($300/year) - 61% savings vs GPT-5.1 only

---

### Option 4: Implement Caching (ADVANCED)
**Goal:** Reduce redundant API calls

#### Opportunities:
1. **Entity Context Caching**
   - Cache enriched entity data for 5-10 minutes
   - Reuse across multiple queries in same session
   - **Savings:** 50-70% on follow-up queries

2. **Device Intelligence Caching**
   - Cache device metadata for 1 hour
   - Rarely changes during session
   - **Savings:** 1,000-3,000 tokens

3. **OpenAI Prompt Caching** (Native feature)
   - OpenAI caches prompts automatically (50% discount on cached input)
   - Effective for repeated system prompts
   - **Savings:** 20-30% on input tokens

**Implementation:**
```python
from functools import lru_cache
from datetime import datetime, timedelta

class EntityContextCache:
    def __init__(self, ttl_seconds=300):
        self.cache = {}
        self.ttl = ttl_seconds
    
    def get(self, key: str) -> Optional[str]:
        if key in self.cache:
            value, timestamp = self.cache[key]
            if datetime.now() - timestamp < timedelta(seconds=self.ttl):
                return value
        return None
    
    def set(self, key: str, value: str):
        self.cache[key] = (value, datetime.now())
```

**Estimated Savings:** 30-50% with effective caching

---

## Final Recommendations

### Immediate Actions (This Week)

1. **Set `ENABLE_ENRICHMENT_CONTEXT=false`** in `.env` ‚úÖ
   - Quick fix to reduce tokens by 2,000-5,000
   - Minimal impact on quality
   - Can be re-enabled selectively per query type

2. **Upgrade to OpenAI Tier 2** ($50/month) ‚úÖ
   - Immediate fix for rate limits
   - Allows time for optimization
   - No code changes needed

3. **Add Token Counting and Logging** üìä
   - Measure actual token usage
   - Identify biggest contributors
   - Track optimization impact

### Short-Term (Next 2 Weeks)

4. **Implement Entity Context Compression**
   - Limit attributes to relevant fields only
   - Summarize capabilities
   - Target: 10,000 tokens max for entity context

5. **Add Token Budget System**
   - Enforce limits per component
   - Truncate if exceeding budget
   - Log warnings when approaching limits

6. **Switch Non-Critical Paths to GPT-4o-mini**
   - YAML generation ‚Üí GPT-4o-mini
   - Entity extraction ‚Üí GPT-4o-mini
   - Category inference ‚Üí GPT-4o-mini
   - Keep suggestion generation on GPT-5.1

### Long-Term (Next Month)

7. **Implement Entity Context Caching**
   - 5-minute TTL for enriched data
   - Redis or in-memory cache
   - Session-aware caching

8. **Add Conversation History Pruning**
   - Keep only last 2-3 turns
   - Summarize older context
   - Archive full history in DB

9. **Monitor and Optimize**
   - Track token usage metrics
   - A/B test GPT-5.1 vs GPT-4o-mini quality
   - Adjust based on actual usage patterns

---

## Implementation Checklist

### Phase 1: Emergency Fix (Today)
- [ ] Set `ENABLE_ENRICHMENT_CONTEXT=false` in infrastructure/env.ai-automation
- [ ] Restart ai-automation-service
- [ ] Test Ask AI endpoint
- [ ] Verify token usage reduced

### Phase 2: Verify Rate Limits (This Week) ‚ö†Ô∏è UPDATED
- [ ] Check OpenAI account dashboard: https://platform.openai.com/account/rate-limits
- [ ] Verify current tier (should be Tier 1 = 500K TPM, not 30K)
- [ ] If still seeing 30K TPM, contact OpenAI support or verify account
- [ ] Update rate limits in config documentation
- [ ] Monitor usage dashboard

### Phase 3: Token Optimization (Next 2 Weeks) ‚úÖ UPDATED
- [ ] Add tiktoken library to requirements.txt
- [ ] Implement token counting utility
- [ ] Add logging to measure token usage per component
- [ ] Implement entity context compression
- [ ] Add token budget enforcement
- [ ] Switch YAML generation to GPT-5 Mini (80% cost savings)
- [ ] Switch classification to GPT-5 Nano (96% cost savings)
- [ ] Test quality impact
- [ ] Implement prompt caching (90% discount on cached inputs)

### Phase 4: Caching (Next Month)
- [ ] Implement entity context cache
- [ ] Add conversation history pruning
- [ ] Implement OpenAI native prompt caching
- [ ] Monitor cache hit rates
- [ ] Measure cost savings

---

## Success Metrics

| Metric | Before | Target | Measured |
|--------|--------|--------|----------|
| Avg Input Tokens | 25,000 | 12,000 | TBD |
| Max Input Tokens | 36,000 | 18,000 | TBD |
| Rate Limit Errors | 5-10/day | 0 | TBD |
| Cost per Request | $0.03925 | $0.020 | TBD |
| Monthly Cost | $64.43 | $25.00 | TBD |
| Response Quality | Baseline | ‚â•95% | TBD |

---

## Cost-Benefit Summary

| Option | Implementation | Cost Change | Token Savings | Recommended |
|--------|---------------|-------------|---------------|-------------|
| **Option 1: Optimize** | 2 weeks | -$155/year | 40-60% | ‚úÖ **YES** |
| **Option 2: Verify Tier** | 1 day | $0 (may already have 500K TPM) | 0% | ‚úÖ **YES** (verify first) |
| **Option 3: GPT-5 Mini/Nano** | 1 week | -$473/year | N/A | ‚úÖ **YES** (hybrid) |
| **Option 4: Caching** | 3 weeks | -$270/year | 30-50% | ‚úÖ **YES** (long-term) |

### Combined Strategy (BEST) ‚úÖ UPDATED:
1. **Verify Tier 1 = 500K TPM** (should already have it - no cost)
2. Implement optimizations over 2 weeks (reduce to $40/month)
3. Switch to hybrid model: GPT-5.1 + GPT-5 Mini + GPT-5 Nano (reduce to $25/month)
4. Add prompt caching (90% discount on cached inputs - final cost: ~$15/month)

**Total Savings:** $49.43/month ($593/year)  
**Net Cost After Optimization:** $15/month vs $64.43/month today (77% savings)

---

## References ‚úÖ VERIFIED (November 2025)

- OpenAI Pricing: https://openai.com/pricing
- GPT-5.1 Documentation: https://platform.openai.com/docs/models/gpt-5
- Rate Limits: https://platform.openai.com/account/rate-limits
- Token Counting: https://github.com/openai/tiktoken
- Rate Limit Updates (Sept 2025): https://ai-primer.com/reports/2025-09-13
- GPT-5.1 Release Info: https://www.thepromptbuddy.com/prompts/gpt-5-1-release

## Key Updates (November 2025) ‚ö†Ô∏è

1. **Pricing Correction:** GPT-5.1 input tokens are $1.25/1M (not $2.50) - 50% cheaper than initial estimate
2. **Rate Limits:** Tier 1 is now 500K TPM (not 30K) - 16x increase in September 2025
3. **New Models:** GPT-5 Mini and GPT-5 Nano available with 80-96% cost savings
4. **Prompt Caching:** 90% discount on cached input tokens ($0.125/1M vs $1.25/1M)
5. **Batch API:** 50% discount for non-time-sensitive tasks

---

**Next Steps:**
1. Review this analysis with team
2. Get approval for Tier 2 upgrade ($50/month)
3. Implement Phase 1 (emergency fix) today
4. Schedule Phase 3 work for next sprint
5. Track metrics and adjust strategy

