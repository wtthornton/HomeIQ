# Ask AI - All OpenAI API Calls

**Last Updated:** January 2025  
**Service:** ai-automation-service (Port 8024)  
**Endpoint:** http://localhost:3001/ask-ai

## Overview

This document lists **ALL** OpenAI API calls that occur in the Ask AI flow, from user query submission to automation creation. Each call is documented with:
- **Purpose**: What the call does
- **Location**: File and function where it's made
- **When**: At what point in the flow it occurs
- **Model**: Which OpenAI model is used
- **Temperature**: Creativity/consistency setting
- **Max Tokens**: Token limit for response
- **Input Format**: What data is sent
- **Output Format**: What data is returned

---

## OpenAI API Call Summary

The Ask AI flow can make **up to 7 different OpenAI API calls**, depending on the scenario:

1. **Entity Extraction** (Optional - if multi-model extraction enabled)
2. **Suggestion Generation** (Always - main query processing)
3. **Question Generation** (Conditional - if clarification needed)
4. **Command Simplification** (Conditional - if test mode used)
5. **YAML Generation** (Conditional - when suggestion approved)
6. **Test Execution Analysis** (Conditional - if test execution analyzed)
7. **Component Restoration** (Conditional - if testing stripped components)

---

## 1. Entity Extraction (Multi-Model Extractor)

**Purpose**: Extract entities (devices, locations, actions) from user query using OpenAI as part of multi-model approach (NER → OpenAI → Pattern)

**File**: `services/ai-automation-service/src/entity_extraction/multi_model_extractor.py`  
**Function**: `_extract_with_openai()`  
**Lines**: 209-224

**When**: During query processing if `entity_extraction_method == "multi_model"`

**API Call:**
```python
response = await openai_client.chat.completions.create(
    model=self.openai_model,  # Default: gpt-4o-mini
    messages=[
        {
            "role": "system",
            "content": "You are a Home Assistant entity extraction expert. Extract entities from user queries for home automation."
        },
        {
            "role": "user",
            "content": prompt  # Contains query + extraction instructions
        }
    ],
    temperature=0.1,  # Low for consistent extraction
    max_completion_tokens=300  # Short output
)
```

**Input Prompt Structure:**
```
Extract entities from this Home Assistant automation query:

QUERY: "{query}"

Extract:
- Room/area names
- Action devices (lights, switches, etc.)
- Trigger conditions (motion, door opens, etc.)
- Actions (turn on, flash, etc.)
- Time references

Return JSON:
{
  "entities": [
    {
      "name": "entity_name",
      "type": "device|location|action|trigger",
      "domain": "light|switch|binary_sensor|...",
      "confidence": 0.0-1.0
    }
  ]
}
```

**Output Format**: JSON with extracted entities

**Frequency**: Optional (only if multi-model extraction enabled)

---

## 2. Suggestion Generation

**Purpose**: Generate automation suggestions from user query and extracted entities

**File**: `services/ai-automation-service/src/api/ask_ai_router.py`  
**Function**: `generate_suggestions_from_query()`  
**Lines**: 3263-4488

**When**: Always during query processing (after entity extraction)

**API Call**: Made via `ModelOrchestrator` or direct `OpenAIClient.generate_with_unified_prompt()`

**File**: `services/ai-automation-service/src/llm/openai_client.py`  
**Function**: `generate_with_unified_prompt()`  
**Lines**: 414-605

**API Call:**
```python
response = await self.client.chat.completions.create(
    model=self.model,  # Default: gpt-4o
    messages=[
        {"role": "system", "content": prompt_dict["system_prompt"]},
        # Optional developer notes for pattern_summary, synergy_context, feedback_hint
        {"role": "user", "content": prompt_dict["user_prompt"]}
    ],
    temperature=0.7,  # Balanced creativity
    max_completion_tokens=600  # For suggestion output
)
```

**System Prompt**: Generated by `UnifiedPromptBuilder` and includes:
- Role: "Home Assistant automation suggestion expert"
- Instructions for generating suggestions
- Entity context and capabilities
- Format requirements

**User Prompt**: Contains:
- Original user query
- Extracted entities with full context
- Entity capabilities (brightness ranges, color modes, etc.)
- Location/area information
- Clarification context (if any)

**Output Format**: JSON or structured text with:
- Multiple automation suggestions
- Each with: title, description, trigger_summary, action_summary, devices_involved, category, priority

**Model Used**: 
- Default: `gpt-4o` (from settings)
- Can be overridden via `ModelOrchestrator` for parallel testing
- Can use different model per query if configured

**Frequency**: Always (1 call per user query)

**Token Usage**: High (typically 3000-8000 input tokens due to entity context)

---

## 3. Question Generation (Clarification)

**Purpose**: Generate clarification questions when query is ambiguous (multiple matching devices, unclear intent, etc.)

**File**: `services/ai-automation-service/src/services/clarification/question_generator.py`  
**Function**: `generate_questions()`  
**Lines**: 66-96

**When**: If `clarification_needed == true` and ambiguities detected

**API Call:**
```python
response = await self.openai_client.client.chat.completions.create(
    model=self.openai_client.model,  # Default: gpt-4o
    messages=[
        {
            "role": "system",
            "content": "You are a Home Assistant automation assistant helping users clarify their automation requests. Generate natural, helpful clarification questions. Respond ONLY with JSON, no markdown formatting."
        },
        {
            "role": "user",
            "content": prompt  # Contains ambiguities + query context
        }
    ],
    temperature=0.3,  # Consistent questions
    max_completion_tokens=400,  # Medium output
    response_format={"type": "json_object"}  # Force JSON
)
```

**Input Prompt Structure:**
```
Generate clarification questions for this ambiguous automation query:

QUERY: "{query}"

AMBIGUITIES DETECTED:
1. Multiple matching devices: [list of devices]
2. Unclear trigger condition: [description]
3. Missing context: [what's missing]

AVAILABLE DEVICES:
[list of devices with entity IDs]

PREVIOUS ANSWERS (if any):
[Q&A history]

Generate 2-4 clarification questions to help user specify their intent.
```

**Output Format**: JSON
```json
{
  "questions": [
    {
      "id": "q1",
      "question_text": "Which lights do you want to control?",
      "question_type": "entity_selection",
      "options": [...]
    }
  ]
}
```

**Frequency**: Conditional (only if clarification needed)

---

## 4. Command Simplification (Test Mode)

**Purpose**: Simplify automation description to extract core command for quick test execution via HA Conversation API

**File**: `services/ai-automation-service/src/api/ask_ai_router.py`  
**Function**: `simplify_for_ha_conversation()`  
**Lines**: 2621-2638

**When**: During test execution to create simple command for HA Conversation API

**API Call:**
```python
response = await openai_client.client.chat.completions.create(
    model=openai_client.model,  # Default: gpt-4o
    messages=[
        {
            "role": "system",
            "content": "You are a command simplification expert. Extract core behaviors from automation descriptions. Return only the simplified command, no explanations."
        },
        {
            "role": "user",
            "content": prompt  # Contains description + simplification instructions
        }
    ],
    temperature=0.1,  # Very low for deterministic extraction
    max_completion_tokens=60,  # Very short - just the command
    top_p=0.9  # Nucleus sampling
)
```

**Input Prompt Structure:**
```
Simplify this automation description to extract the core command:

DESCRIPTION: "{description}"

REMOVE:
- Time constraints (after 5pm, before sunset)
- Interval patterns (every 30 seconds)
- Conditional logic (only if, but only when)

KEEP:
- Core action (flash, turn on, dim)
- Essential trigger (when door opens, when motion detected)
- Target devices (office lights, kitchen lights)

Return ONLY the simplified command (max 20 words).
```

**Output Format**: Plain text (simplified command)

**Frequency**: Conditional (only if test mode executed)

---

## 5. YAML Generation (Approval)

**Purpose**: Generate Home Assistant automation YAML from approved suggestion

**File**: `services/ai-automation-service/src/api/ask_ai_router.py`  
**Function**: `generate_automation_yaml()`  
**Lines**: 2393-2413

**When**: When user approves a suggestion

**API Call:**
```python
response = await yaml_client.client.chat.completions.create(
    model=yaml_client.model,  # Default: gpt-4o (can be different model via settings)
    messages=[
        {
            "role": "system",
            "content": (
                "You are a Home Assistant 2025 YAML automation expert. "
                "Your output is production-ready YAML that passes Home Assistant validation. "
                "You NEVER invent entity IDs - you ONLY use entity IDs from the validated list. "
                "You ALWAYS use 2025 format: triggers: (plural), actions: (plural), action: (not service:). "
                "Return ONLY valid YAML starting with 'id:' - NO markdown, NO explanations."
            )
        },
        {
            "role": "user",
            "content": prompt  # Comprehensive prompt with entity validation, examples, rules
        }
    ],
    temperature=yaml_temperature,  # Default: 0.1 (very low for consistency)
    max_completion_tokens=2000  # Large output for complex automations
)
```

**Input Prompt Structure:**
```
TASK: Generate Home Assistant 2025 automation YAML from this request.

USER REQUEST: "{original_query}"

AUTOMATION SPECIFICATION:
- Description: {suggestion.description}
- Trigger: {suggestion.trigger_summary}
- Action: {suggestion.action_summary}
- Devices: {suggestion.devices_involved}

VALIDATED ENTITIES (ALL verified to exist in Home Assistant):
- {entity_name}: {entity_id}
- [explicit mappings with validation warnings]

ENTITY CONTEXT (Complete Information):
{enriched_entity_context_json}
[capabilities, ranges, available services]

HOME ASSISTANT YAML FORMAT (CURRENT STANDARD):
[examples and rules]

CRITICAL RULES:
1. Use ONLY validated entity IDs
2. Use 2025 format (trigger:/action: singular at top level)
3. Use platform:/service: in items
4. [more rules...]
```

**Output Format**: YAML string (raw, no markdown wrapping)
```yaml
id: 'unique_id_timestamp_uuid'
alias: "Descriptive Name"
description: "What it does"
mode: single
trigger:
  - platform: state
    entity_id: binary_sensor.office_motion
    to: 'on'
action:
  - service: light.turn_on
    target:
      entity_id: light.office_ceiling
    data:
      brightness_pct: 100
```

**Model Used**: 
- Default: `gpt-4o` (from settings)
- Can be overridden via `YAML_GENERATION_MODEL` setting
- Can use parallel testing with multiple models

**Frequency**: Conditional (only when suggestion approved)

**Token Usage**: Very High (typically 4000-12000 input tokens due to comprehensive prompt)

---

## 6. Test Execution Analysis

**Purpose**: Analyze test execution results to provide structured feedback

**File**: `services/ai-automation-service/src/api/ask_ai_router.py`  
**Function**: `TestAnalyzer.analyze_execution()`  
**Lines**: 6860-6878

**When**: After test execution if analysis is requested

**API Call:**
```python
response = await self.client.client.chat.completions.create(
    model=self.client.model,  # Default: gpt-4o
    messages=[
        {
            "role": "system",
            "content": "You are a test automation analysis expert. Analyze execution results and provide structured feedback in JSON format only."
        },
        {
            "role": "user",
            "content": prompt  # Contains test YAML + state validation + execution logs
        }
    ],
    temperature=0.2,  # Low for consistent analysis
    max_completion_tokens=400,  # Medium output
    response_format={"type": "json_object"}  # Force JSON
)
```

**Input Prompt Structure:**
```
Analyze this test automation execution and provide structured feedback.

TEST YAML:
{test_yaml}

STATE VALIDATION RESULTS:
- Entities checked: {total_count}
- Entities changed: {changed_count}
- [validation details]

ENTITY CHANGES:
{entity_changes_json}

EXECUTION LOGS:
{execution_logs}

TASK: Analyze and determine:
1. Did the automation execute successfully?
2. Were expected state changes detected?
3. Any issues or warnings?
4. Recommendations?
```

**Output Format**: JSON
```json
{
  "success": true/false,
  "issues": ["List of issues found"],
  "recommendations": ["List of recommendations"],
  "confidence": 0.0-1.0
}
```

**Frequency**: Conditional (only if test analysis requested)

---

## 7. Component Restoration (Testing)

**Purpose**: Restore timing components (delays, repeats) that were stripped during testing, ensuring correct structure

**File**: `services/ai-automation-service/src/api/ask_ai_router.py`  
**Function**: `restore_components_for_approval()`  
**Lines**: 7466-7478

**When**: If components were stripped during test mode and need restoration before approval

**API Call:**
```python
response = await openai_client.client.chat.completions.create(
    model=openai_client.model,  # Default: gpt-4o
    messages=[
        {
            "role": "system",
            "content": "You are an automation expert. Restore timing, delay, and repeat components that were removed for testing, ensuring they match the original user intent. Pay special attention to nested components (delays within repeats) and restore them with correct structure."
        },
        {
            "role": "user",
            "content": prompt  # Contains original query, suggestion, stripped components
        }
    ],
    temperature=0.1,  # Very low for deterministic restoration
    max_completion_tokens=500,  # Medium output
    response_format={"type": "json_object"}  # Force JSON
)
```

**Input Prompt Structure:**
```
Restore these automation components that were stripped during testing.

ORIGINAL USER QUERY:
"{original_query}"

ORIGINAL SUGGESTION:
Description: {suggestion.description}
Trigger: {suggestion.trigger_summary}
Action: {suggestion.action_summary}

STRIPPED COMPONENTS TO RESTORE:
- delay: 00:00:05 (confidence: 0.9)
- repeat: 3 times (confidence: 0.85)
[component list with confidence scores]

NESTED COMPONENTS DETECTED:
[if any delays within repeats]

TASK:
1. Analyze original query context
2. Identify nested components
3. Restore components in correct structure
4. Validate intent match
```

**Output Format**: JSON
```json
{
  "restored": true/false,
  "restored_components": ["delay", "repeat"],
  "restoration_details": ["detailed descriptions"],
  "nested_components_restored": ["delay within repeat"],
  "restoration_structure": "description of hierarchy",
  "confidence": 0.0-1.0,
  "intent_match": true/false,
  "intent_validation": "explanation"
}
```

**Frequency**: Conditional (only if components were stripped during testing)

---

## Complete Call Flow Diagram

```
USER SUBMITS QUERY
│
├─► [1] Entity Extraction (Optional - if multi-model)
│   └─► OpenAI: Extract entities from query
│
├─► [2] Suggestion Generation (Always)
│   └─► OpenAI: Generate automation suggestions
│
└─► [3] Question Generation (Conditional - if clarification needed)
    └─► OpenAI: Generate clarification questions
    │
    └─► USER ANSWERS QUESTIONS
        │
        └─► [2] Suggestion Generation (Again with clarification context)
            └─► OpenAI: Generate refined suggestions

USER APPROVES SUGGESTION
│
├─► [5] YAML Generation (Always on approval)
│   └─► OpenAI: Generate automation YAML
│
└─► [Optional: Test Mode]
    │
    ├─► [4] Command Simplification (If test executed)
    │   └─► OpenAI: Simplify description for HA Conversation API
    │
    ├─► [6] Test Execution Analysis (If analysis requested)
    │   └─► OpenAI: Analyze test results
    │
    └─► [7] Component Restoration (If components stripped)
        └─► OpenAI: Restore delays/repeats before approval
```

---

## Token Usage Summary

### 2025 OpenAI Pricing (as of November 2025)

**GPT-5.1 (Released November 12, 2025):**
- **GPT-5.1 Instant/Thinking:**
  - Input: $1.25 per 1M tokens ($0.00000125 per token)
  - Output: $10.00 per 1M tokens ($0.00001 per token)
  - Cached Input: $0.125 per 1M tokens (90% discount on repeated prompts)
  - **Best for:** High-quality automation suggestions and YAML generation

- **GPT-5.1 Mini:**
  - Input: $0.25 per 1M tokens ($0.00000025 per token)
  - Output: $2.00 per 1M tokens ($0.000002 per token)
  - Cached Input: $0.025 per 1M tokens (90% discount)
  - **Best for:** Entity extraction, classification, and lighter tasks

- **GPT-5.1 Nano:**
  - Input: $0.05 per 1M tokens ($0.00000005 per token)
  - Output: $0.40 per 1M tokens ($0.0000004 per token)
  - **Best for:** Simple text tasks, basic extraction

**GPT-4o (Previous Generation):**
- Input: $2.50 per 1M tokens ($0.0000025 per token)
- Output: $10.00 per 1M tokens ($0.00001 per token)

**GPT-4o Mini (Previous Generation):**
- Input: $0.15 per 1M tokens ($0.00000015 per token)
- Output: $0.60 per 1M tokens ($0.0000006 per token)

### Typical Query Flow (No Clarification, No Testing)

#### Using GPT-4o Models (Current Default)

| Call | Model | Input Tokens | Output Tokens | Total Tokens | Cost (GPT-4o) |
|------|-------|-------------|---------------|--------------|---------------|
| Entity Extraction (if enabled) | gpt-4o-mini | 200-500 | 100-300 | 300-800 | $0.0001-0.0003 |
| Suggestion Generation | gpt-4o | 3000-8000 | 500-1500 | 3500-9500 | $0.013-0.035 |
| YAML Generation | gpt-4o | 4000-12000 | 800-2000 | 4800-14000 | $0.018-0.050 |
| **Total** | **Mixed** | **7200-20500** | **1400-3800** | **8600-24300** | **$0.031-0.085** |

#### Using GPT-5.1 Models (Recommended Upgrade)

| Call | Model | Input Tokens | Output Tokens | Total Tokens | Cost (GPT-5.1) | Savings vs GPT-4o |
|------|-------|-------------|---------------|--------------|----------------|-------------------|
| Entity Extraction (if enabled) | gpt-5.1-mini | 200-500 | 100-300 | 300-800 | $0.0001-0.0002 | ~33% savings |
| Suggestion Generation | gpt-5.1 | 3000-8000 | 500-1500 | 3500-9500 | $0.006-0.017 | ~50% savings |
| YAML Generation | gpt-5.1 | 4000-12000 | 800-2000 | 4800-14000 | $0.009-0.025 | ~50% savings |
| **Total** | **Mixed** | **7200-20500** | **1400-3800** | **8600-24300** | **$0.015-0.042** | **~50% savings** |

#### Using GPT-5.1 with Prompt Caching (90% Discount on Repeated Inputs)

| Call | Model | Input Tokens | Cached Input Cost | Output Tokens | Total Cost (Cached) | Savings vs Non-Cached |
|------|-------|-------------|-------------------|---------------|---------------------|---------------------|
| Suggestion Generation | gpt-5.1 | 3000-8000 | $0.0004-0.001 | 500-1500 | $0.005-0.011 | Additional ~35% savings |
| YAML Generation | gpt-5.1 | 4000-12000 | $0.0005-0.0015 | 800-2000 | $0.008-0.017 | Additional ~35% savings |
| **Total (with caching)** | **gpt-5.1** | **7000-20000** | **$0.0009-0.0025** | **1300-3500** | **$0.014-0.029** | **~60% total savings vs GPT-4o** |

**Cost Breakdown:**
- **GPT-4o**: Without entity extraction: **$0.031-0.085 per query**
- **GPT-5.1**: Without entity extraction: **$0.015-0.042 per query** (~50% savings)
- **GPT-5.1 (Cached)**: Without entity extraction: **$0.014-0.029 per query** (~60% savings)
- With entity extraction: Minimal additional cost (same across all models)

### With Clarification

| Call | Model | Input Tokens | Output Tokens | Total Tokens | Cost (2025) |
|------|-------|-------------|---------------|--------------|-------------|
| Suggestion Generation (initial) | gpt-4o | 3000-8000 | 500-1500 | 3500-9500 | $0.013-0.035 |
| Question Generation | gpt-4o | 800-1500 | 200-400 | 1000-1900 | $0.004-0.008 |
| Suggestion Generation (refined) | gpt-4o | 3500-9000 | 500-1500 | 4000-10500 | $0.014-0.038 |
| **Total (clarification)** | **gpt-4o** | **7300-18500** | **1200-3400** | **8500-21900** | **$0.031-0.081** |

### With Testing

| Call | Model | Input Tokens | Output Tokens | Total Tokens | Cost (2025) |
|------|-------|-------------|---------------|--------------|-------------|
| Command Simplification | gpt-4o | 300-600 | 20-60 | 320-660 | $0.001-0.002 |
| Test Analysis | gpt-4o | 1500-3000 | 200-400 | 1700-3400 | $0.006-0.011 |
| Component Restoration | gpt-4o | 1000-2000 | 200-500 | 1200-2500 | $0.004-0.008 |
| **Total (testing)** | **gpt-4o** | **2800-5600** | **420-960** | **3220-6560** | **$0.011-0.021** |

### Complete Flow Examples

**Example 1: Simple Query → Approve** (Most Common)

| Model | Suggestion Generation | YAML Generation | Total | Savings vs GPT-4o |
|-------|----------------------|-----------------|-------|-------------------|
| GPT-4o | $0.013-0.035 | $0.018-0.050 | **$0.031-0.085** (~$0.06) | Baseline |
| GPT-5.1 | $0.006-0.017 | $0.009-0.025 | **$0.015-0.042** (~$0.03) | ~50% savings |
| GPT-5.1 (Cached) | $0.005-0.011 | $0.008-0.017 | **$0.014-0.029** (~$0.022) | ~60% savings |

**Example 2: Query → Clarify → Approve**

| Model | Initial Suggestion | Question Gen | Refined Suggestion | YAML Generation | Total | Savings |
|-------|-------------------|--------------|-------------------|-----------------|-------|---------|
| GPT-4o | $0.013-0.035 | $0.004-0.008 | $0.014-0.038 | $0.018-0.050 | **$0.049-0.131** (~$0.09) | Baseline |
| GPT-5.1 | $0.006-0.017 | $0.002-0.004 | $0.007-0.019 | $0.009-0.025 | **$0.024-0.065** (~$0.045) | ~50% savings |
| GPT-5.1 (Cached) | $0.005-0.011 | $0.001-0.002 | $0.006-0.012 | $0.008-0.017 | **$0.020-0.042** (~$0.031) | ~60% savings |

**Example 3: Query → Test → Analyze → Approve**

| Model | Suggestion Gen | Command Simplify | Test Analysis | Component Restore | YAML Gen | Total | Savings |
|-------|---------------|------------------|---------------|-------------------|----------|-------|---------|
| GPT-4o | $0.013-0.035 | $0.001-0.002 | $0.006-0.011 | $0.004-0.008 | $0.018-0.050 | **$0.042-0.106** (~$0.07) | Baseline |
| GPT-5.1 | $0.006-0.017 | $0.0005-0.001 | $0.003-0.005 | $0.002-0.004 | $0.009-0.025 | **$0.021-0.052** (~$0.037) | ~50% savings |
| GPT-5.1 (Cached) | $0.005-0.011 | $0.0004-0.001 | $0.002-0.004 | $0.002-0.003 | $0.008-0.017 | **$0.018-0.036** (~$0.027) | ~60% savings |

**Monthly Cost Estimates** (assuming 100 queries/day):

| Scenario | GPT-4o | GPT-5.1 | GPT-5.1 (Cached) | Savings |
|----------|--------|---------|------------------|---------|
| Simple queries only | $93-255/month (~$174/month) | $45-127/month (~$86/month) | $42-110/month (~$76/month) | ~50-56% |
| With 20% clarification | $108-293/month (~$200/month) | $54-146/month (~$100/month) | $50-127/month (~$89/month) | ~50-55% |
| With 30% testing | $104-270/month (~$187/month) | $52-135/month (~$94/month) | $48-117/month (~$83/month) | ~50-56% |

**Model Selection Recommendations:**
- **Production (Quality First)**: Use **GPT-5.1** for suggestions and YAML generation (better quality, 50% cost savings vs GPT-4o)
- **Cost Optimization**: Use **GPT-5.1 with prompt caching** when possible (60% savings vs GPT-4o)
- **Light Tasks**: Use **GPT-5.1 Mini** for entity extraction (33% cheaper than GPT-4o-mini)
- **Ultra-Low Cost**: Use **GPT-5.1 Nano** for very simple extraction tasks (80% cheaper than GPT-4o-mini)

**Note**: Costs calculated using 2025 OpenAI pricing (November 2025). GPT-5.1 was released November 12, 2025. Actual costs may vary based on:
- Model used (gpt-4o, gpt-5.1, or older models)
- Actual token usage (can vary based on entity count, query complexity)
- Batch API usage (50% discount available for batch processing)
- Volume discounts (if applicable)

---

## Model Configuration

Models are configured via `settings` in `services/ai-automation-service/src/config.py`:

```python
# Recommended 2025 models (GPT-5.1 - 50% cost savings vs GPT-4o)
OPENAI_MODEL = "gpt-5.1"  # Main model for suggestions (or "gpt-5.1-instant")
OPENAI_YAML_MODEL = "gpt-5.1"  # YAML generation (50% cheaper than GPT-4o)
CLASSIFICATION_MODEL = "gpt-5.1-mini"  # For classification tasks

# Entity extraction (if multi-model enabled)
ENTITY_EXTRACTION_MODEL = "gpt-5.1-mini"  # Lighter model (or "gpt-5.1-nano" for ultra-light)

# Legacy models (still supported)
# OPENAI_MODEL = "gpt-4o"  # Previous generation
# OPENAI_YAML_MODEL = "gpt-4o"  # Previous generation
# CLASSIFICATION_MODEL = "gpt-4o-mini"  # Previous generation
# ENTITY_EXTRACTION_MODEL = "gpt-4o-mini"  # Previous generation
```

**Model Selection Strategy:**

**Recommended (2025):**
- **Heavy tasks** (suggestions, YAML): `gpt-5.1` (best quality, 50% cost savings vs GPT-4o)
- **Light tasks** (extraction, classification): `gpt-5.1-mini` (better quality, similar cost to GPT-4o-mini)
- **Critical tasks** (YAML generation): `gpt-5.1` with prompt caching (60% savings vs GPT-4o)
- **Ultra-light tasks**: `gpt-5.1-nano` (80% cheaper than GPT-4o-mini)

**Legacy (Still Supported):**
- **Heavy tasks** (suggestions, YAML): `gpt-4o` (previous generation)
- **Light tasks** (extraction, classification): `gpt-4o-mini` (previous generation)

**Note**: Models can be configured via `YAML_GENERATION_MODEL` and other settings. GPT-5.1 models offer better quality and cost efficiency than GPT-4o models.

---

## Error Handling

All OpenAI API calls include retry logic via `tenacity`:

```python
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type((Exception,)),
    reraise=True
)
```

**Retry Strategy:**
- **Max Attempts**: 3
- **Backoff**: Exponential (2s, 4s, 8s)
- **On Failure**: Log error, return fallback if available, or raise exception

**Fallback Behavior:**
- **Entity Extraction**: Falls back to pattern matching
- **Suggestion Generation**: Returns error (no fallback)
- **YAML Generation**: Returns error (no fallback)
- **Question Generation**: Generates simple questions from ambiguities
- **Command Simplification**: Uses regex-based fallback
- **Test Analysis**: Returns default success response
- **Component Restoration**: Returns original suggestion (no restoration)

---

## Cost Optimization

1. **Upgrade to GPT-5.1**: **50% cost savings** vs GPT-4o for same quality tasks
   - GPT-5.1: $1.25/1M input tokens (vs GPT-4o: $2.50/1M)
   - Same output pricing: $10.00/1M tokens
   - **Recommended**: Use GPT-5.1 for all new deployments

2. **Enable Prompt Caching (GPT-5.1)**: **90% discount** on repeated inputs
   - Cached inputs: $0.125/1M tokens (vs $1.25/1M regular)
   - **Best for**: Suggestion generation and YAML generation (repeated system prompts)
   - **Total savings**: ~60% vs GPT-4o when caching enabled

3. **Model Selection by Task**:
   - Heavy tasks: GPT-5.1 (best quality + 50% savings)
   - Light tasks: GPT-5.1 Mini (33% cheaper than GPT-4o-mini)
   - Ultra-light: GPT-5.1 Nano (80% cheaper than GPT-4o-mini)

4. **Entity Context Filtering**: Filters entity context by location/device name to reduce prompt size
5. **Token Budgeting**: Tracks token usage and warns on threshold
6. **Parallel Testing**: Can test multiple models simultaneously (if enabled)

**Optimization Strategies:**
- **Immediate**: Upgrade to GPT-5.1 models (50% savings)
- **High Impact**: Enable prompt caching for GPT-5.1 (additional 35% savings on cached inputs)
- **Long-term**: Implement batch API for non-time-critical tasks (50% discount available)
- **Future**: Streaming responses for long outputs to reduce latency

---

## Monitoring

All OpenAI API calls are logged with:
- **Token usage**: Input, output, total tokens
- **Cost tracking**: Per-call and aggregate costs
- **Model used**: Which model made the call
- **Endpoint tracking**: Which endpoint triggered the call
- **Timing**: Request/response times

**Stats Available:**
- Total tokens used
- Total cost (USD)
- Per-endpoint breakdown
- Per-model breakdown
- Average cost per call

Accessible via debug panel or API endpoint.

