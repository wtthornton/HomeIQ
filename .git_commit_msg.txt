feat: Implement token optimization for Ask AI clarification flow

- Option 1: Aggressive entity compression (3k-5k token savings)
  * Reduced max_entity_context_tokens from 10,000 to 7,000
  * Enhanced compression with effect_list summarization
  * Removed verbose device_intelligence details
  * Added relevance-based prioritization before compression

- Option 3: Enhanced relevance-based entity filtering (4k-6k token savings)
  * Added _score_entities_by_relevance() function
  * Scores entities by query + clarification answers
  * Keeps top 25 most relevant entities before filtering
  * Prioritizes entities from clarification answers

- Improved error handling for 504 timeout errors
  * Returns structured error objects instead of strings
  * Better frontend error parsing and display

Expected impact:
- Token usage: 25,557 -> ~14,557-18,557 tokens (28-43% reduction)
- Available for response: 4,443 -> 11,443-15,443 tokens
- Should resolve timeout issues with clarification submissions
