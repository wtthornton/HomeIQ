"""
Mock OpenAI Client

Deterministic YAML/suggestion generation without API calls.
Maintains same interface as production OpenAIClient.
"""

import json
import logging
import re
from typing import Any

logger = logging.getLogger(__name__)


class MockOpenAIClient:
    """
    Mock OpenAI client for simulation.
    
    Generates deterministic responses based on input prompts.
    No actual API calls are made.
    """

    def __init__(self, api_key: str = "mock-key", model: str = "gpt-4o", enable_token_counting: bool = True):
        """
        Initialize mock OpenAI client.
        
        Args:
            api_key: Mock API key (not used)
            model: Model name (for logging)
            enable_token_counting: Enable token counting (mock)
        """
        self.api_key = api_key
        self.model = model
        self.total_tokens_used = 0
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost_usd = 0.0
        self.last_usage = None
        self._token_counter_enabled = enable_token_counting
        
        # Endpoint-level tracking (in-memory)
        self.endpoint_stats: dict[str, dict[str, Any]] = {}
        
        logger.info(f"MockOpenAIClient initialized with model={model}")

    async def generate_with_unified_prompt(
        self,
        prompt_dict: dict[str, str],
        temperature: float = 0.7,
        max_tokens: int = 600,
        output_format: str = "yaml",  # "yaml" | "description" | "json"
        endpoint: str | None = None,
        gpt51_use_case: str | None = None
    ) -> dict:
        """
        Generate deterministic response based on prompt.
        
        Args:
            prompt_dict: {"system_prompt": ..., "user_prompt": ...}
            temperature: Creativity level (not used in mock)
            max_tokens: Response limit (not used in mock)
            output_format: Expected output format
            endpoint: Endpoint name (for tracking)
            gpt51_use_case: GPT-5.1 use case (not used in mock)
            
        Returns:
            Parsed response based on output_format
        """
        user_prompt = prompt_dict.get("user_prompt", "")
        system_prompt = prompt_dict.get("system_prompt", "")
        
        # Generate deterministic response based on output_format
        if output_format == "yaml":
            result = self._generate_yaml_response(user_prompt, system_prompt)
        elif output_format == "description":
            result = self._generate_description_response(user_prompt, system_prompt)
        elif output_format == "json":
            result = self._generate_json_response(user_prompt, system_prompt)
        else:
            result = {"error": f"Unknown output_format: {output_format}"}

        # Mock token usage
        input_tokens = len(user_prompt.split()) * 2  # Rough estimate
        output_tokens = len(str(result).split()) * 2
        total_tokens = input_tokens + output_tokens
        
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
        self.total_tokens_used += total_tokens
        self.total_cost_usd += total_tokens * 0.00001  # Mock cost
        
        self.last_usage = {
            'prompt_tokens': input_tokens,
            'completion_tokens': output_tokens,
            'total_tokens': total_tokens,
            'cost_usd': total_tokens * 0.00001
        }

        # Track endpoint stats
        if endpoint:
            if endpoint not in self.endpoint_stats:
                self.endpoint_stats[endpoint] = {
                    'calls': 0,
                    'tokens': 0,
                    'cost_usd': 0.0
                }
            self.endpoint_stats[endpoint]['calls'] += 1
            self.endpoint_stats[endpoint]['tokens'] += total_tokens
            self.endpoint_stats[endpoint]['cost_usd'] += total_tokens * 0.00001

        logger.debug(f"Generated mock response (format={output_format}, tokens={total_tokens})")
        return result

    def _generate_yaml_response(self, user_prompt: str, system_prompt: str) -> dict:
        """Generate deterministic YAML automation."""
        # Extract entity IDs from prompt
        entity_ids = re.findall(r'entity_id[:\s]+([^\s,]+)', user_prompt)
        if not entity_ids:
            entity_ids = re.findall(r'([a-z_]+\.(?:office|living_room|bedroom|kitchen))', user_prompt)
        
        if not entity_ids:
            entity_ids = ["light.office"]  # Default
        
        entity_id = entity_ids[0]
        
        # Generate simple automation YAML
        yaml_content = f"""alias: Mock Automation
description: Automation generated by simulation
trigger:
  - platform: time
    at: "07:00:00"
action:
  - service: {entity_id.split('.')[0]}.turn_on
    target:
      entity_id: {entity_id}
mode: single
"""
        
        return {
            "automation_yaml": yaml_content,
            "alias": "Mock Automation",
            "description": "Automation generated by simulation framework",
            "rationale": "Based on detected usage patterns",
            "category": "convenience",
            "priority": "medium"
        }

    def _generate_description_response(self, user_prompt: str, system_prompt: str) -> dict:
        """Generate deterministic description-only response."""
        # Extract device/entity from prompt
        device_match = re.search(r'(?:device|entity)[:\s]+([^\s,]+)', user_prompt, re.IGNORECASE)
        device = device_match.group(1) if device_match else "device"
        
        return {
            "title": f"Automation for {device}",
            "description": f"Turn on {device} at 7 AM based on detected usage patterns.",
            "rationale": "This automation was detected from historical usage data.",
            "category": "convenience",
            "priority": "medium"
        }

    def _generate_json_response(self, user_prompt: str, system_prompt: str) -> list[dict]:
        """Generate deterministic JSON response (list of suggestions)."""
        # Extract entities from prompt
        entities = re.findall(r'([a-z_]+\.(?:office|living_room|bedroom|kitchen))', user_prompt)
        if not entities:
            entities = ["light.office"]
        
        suggestions = []
        for i, entity_id in enumerate(entities[:3], 1):  # Max 3 suggestions
            suggestions.append({
                "description": f"Automation {i} for {entity_id}",
                "trigger_summary": "Time-based trigger at 7 AM",
                "action_summary": f"Turn on {entity_id}",
                "devices_involved": [entity_id],
                "capabilities_used": ["on_off"],
                "confidence": 0.8
            })
        
        return suggestions

    def reset_usage_stats(self) -> None:
        """Reset usage statistics."""
        self.total_tokens_used = 0
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_cost_usd = 0.0
        self.last_usage = None
        self.endpoint_stats = {}
        logger.info("Mock OpenAI usage statistics reset")

