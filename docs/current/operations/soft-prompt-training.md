# Soft Prompt Training Guide

This document explains how to maintain and retrain the Ask AI soft prompt on a
single-user NUC deployment.

## Overview

- The AI Automation service loads soft prompt artifacts from
  `data/ask_ai_soft_prompt` (configurable via the Settings UI or environment).
- Training data comes from `ask_ai_queries` in `data/ai_automation.db`, using the
  highest-confidence suggestion per query.
- A new training run can be started from the Admin UI or directly via the CLI
  helper script.

## Prerequisites

- Containers must include `transformers`, `torch` (CPU wheel), and `peft`
  packages (already listed in `services/ai-automation-service/requirements.txt`).
- Ensure the database volume is mounted so the trainer can read
  `data/ai_automation.db` and write artifacts back to `data/ask_ai_soft_prompt`.
- Collect labelled Ask AI sessions (approved suggestions) to obtain meaningful
  training samples.

## Launching Training from the Admin UI

1. Navigate to `http://localhost:3001/admin`.
2. In **Training & Model Maintenance**, press **ðŸš€ Start Training**.
3. The job runs asynchronously; the table beneath the button shows status,
   sample count, loss, and any error output. The button is disabled while a run
   is active.
4. On completion, the latest artifacts are symlinked to
   `data/ask_ai_soft_prompt/latest` and will be picked up automatically by the
   backend once the job finishes.

## Launching Training from the CLI

Run the helper script inside the `ai-automation-service` container or on the
host (with the virtualenv activated):

```sh
python scripts/train_soft_prompt.py \
  --db-path data/ai_automation.db \
  --output-dir data/ask_ai_soft_prompt \
  --max-samples 2000 \
  --epochs 3
```

Key options:

- `--base-model`: defaults to `google/flan-t5-small` for CPU-friendly tuning.
- `--run-id` / `--run-directory`: override the autogenerated run folder name.
- `--max-samples`: cap the number of labelled conversations to keep runtimes
  predictable during experimentation.

Metadata for each run is written to `training_run.json` in the run directory.

## Runtime Settings

Update soft prompt behavior from the Settings UI or via `/api/v1/settings`:

- `softPromptEnabled`: toggles fallback usage.
- `softPromptModelDir`: base directory containing trained artifacts.
- `softPromptConfidenceThreshold`: confidence level required before a fallback
  suggestion is injected (0â€“1).

Changes are validated server-sideâ€”model directories must exist when the feature
is enabledâ€”and adapters are reloaded automatically without a restart.

## Training History API

- `GET /api/v1/admin/training/runs`: returns recent runs.
- `POST /api/v1/admin/training/trigger`: queues a new training job; returns 409
  if one is already running.

Both endpoints power the Admin UI dashboard and can be used for automation or
monitoring.

## Troubleshooting

### Common Issues

#### Training Run Fails Immediately

**Symptoms:** Training run fails within 10-30 seconds, status shows "failed"

**Possible Causes:**
1. **No training data available**: The database has no labelled Ask AI queries
   - **Solution**: Ensure you have approved suggestions in the database. Check with:
     ```sql
     SELECT COUNT(*) FROM ask_ai_queries WHERE suggestions IS NOT NULL;
     ```

2. **Model not cached and network unavailable**: The script tries to download the model but fails
   - **Solution**: Ensure network connectivity to HuggingFace Hub, or pre-download the model

3. **Missing dependencies**: Required packages not installed
   - **Solution**: Verify `transformers`, `torch`, and `peft` are installed:
     ```bash
     pip list | grep -E "(transformers|torch|peft)"
     ```

4. **Memory constraints**: Out of memory during model loading or dataset preparation
   - **Solution**: Reduce `--max-samples` or `--batch-size` parameters

#### Error Messages

- **Full error details** are available in the Admin UI:
  - Click "Show full error" to expand truncated messages
  - Click "ðŸ“‹ View in modal" to see complete error with copy functionality
- **Error logs** are also captured in the backend service logs
- **Training script output** (stdout/stderr) is stored in `training_runs.error_message` (up to 5000 chars)

#### Model Loading Issues

The training script will:
1. First attempt to load from cache (configured via `HF_HOME` environment variable)
2. If not found, automatically download from HuggingFace Hub
3. Cache the model for future runs

**Cache location**: Set via `HF_HOME` environment variable (defaults to `~/.cache/huggingface/`)

### Debugging Failed Runs

1. **Check the error message** in the Admin UI Notes column
2. **View full error** using the "View in modal" button
3. **Check backend logs** for detailed error output:
   ```bash
   docker logs ai-automation-service | grep -A 50 "Training script failed"
   ```
4. **Run training manually** to reproduce:
   ```bash
   python scripts/train_soft_prompt.py \
     --db-path data/ai_automation.db \
     --output-dir data/ask_ai_soft_prompt \
     --max-samples 10 \
     --epochs 1
   ```

