# Epic 26: AI Automation UI E2E Test Coverage - Brownfield Enhancement

## Epic Goal

Implement comprehensive end-to-end tests for the AI Automation Suggestions engine UI, covering all critical user workflows from suggestion browsing to deployment, ensuring the complete user journey works reliably and meets quality standards.

## Epic Description

### Existing System Context

**Current Testing State:**
- Unit tests: 100% coverage (56/56 passing) in `services/ai-automation-service/tests/`
- Visual tests: Puppeteer tests for UI rendering only
- E2E tests: **ZERO** for AI automation workflows
- Backend: Comprehensive integration tests for OpenAI, patterns, device intelligence

**AI Automation System:**
- Frontend: React UI at localhost:3001 (Dashboard, Patterns, Deployed, Settings)
- Backend: ai-automation-service at localhost:8018
- Features: Pattern detection, device intelligence, AI-generated suggestions, HA deployment
- Daily job: 3 AM batch analysis generating ~10 suggestions
- Key workflows: Browse → Approve → Deploy → Verify

**Technology Stack:**
- E2E Framework: Playwright 1.56.0 with TypeScript
- Frontend: React 18.2.0, React Router, Zustand state management
- Backend: FastAPI, SQLite, OpenAI GPT-4o-mini
- Integration: Home Assistant MQTT, InfluxDB queries

### Enhancement Details

**What's being added/changed:**

Six critical end-to-end test suites covering the complete user journey:

1. **Suggestion Approval Workflow (Story 26.1)**
   - Browse suggestions on Dashboard
   - Filter by category, confidence, pattern type
   - View suggestion details
   - Approve suggestion
   - Deploy to Home Assistant
   - Verify in Deployed tab

2. **Rejection & Feedback Workflow (Story 26.2)**
   - View suggestion
   - Reject with reason
   - Verify suggestion hidden
   - Check feedback recorded
   - Verify similar suggestions filtered

3. **Pattern Visualization & Analysis (Story 26.3)**
   - Navigate to Patterns tab
   - View time-of-day patterns
   - View co-occurrence patterns
   - Filter by device, time, confidence
   - See readable device names (not hashes)
   - Chart interactions and tooltips

4. **Manual Analysis Trigger & Real-Time Updates (Story 26.4)**
   - Trigger manual analysis via API
   - Monitor progress indicator
   - Wait for completion
   - Verify new suggestions appear
   - Check MQTT notification sent
   - Validate UI auto-refresh

5. **Device Intelligence Features (Story 26.5)**
   - View device utilization metrics
   - See underutilized features
   - Get feature-based suggestions
   - View capability discovery status
   - Zigbee2MQTT integration validation

6. **Settings & Configuration (Story 26.6)**
   - Update OpenAI API key
   - Configure Home Assistant connection
   - Set analysis schedule preferences
   - Save configuration
   - Verify persistence
   - Test validation errors

**How it integrates:**
- Uses test infrastructure from Epic 25
- Leverages Page Object Models
- Follows Playwright best practices (Context7 research)
- Reuses existing Docker test helpers
- Integrates with `run-docker-tests.sh`

**Success criteria:**
1. All 6 test suites implemented and passing
2. Minimum 95% coverage of critical user workflows
3. Tests run in <5 minutes (parallel execution)
4. Zero flaky tests (deterministic execution)
5. All tests use web-first assertions
6. Comprehensive error scenario coverage

### Stories

1. **Story 26.1:** Suggestion Approval & Deployment E2E Tests
   - Complete workflow: Browse → Approve → Deploy → Verify
   - Filter and search functionality
   - Success and error states
   - Home Assistant integration validation

2. **Story 26.2:** Suggestion Rejection & Feedback E2E Tests
   - Rejection workflow with feedback
   - Suggestion hiding behavior
   - Feedback persistence
   - Similar suggestion filtering

3. **Story 26.3:** Pattern Visualization E2E Tests
   - Pattern browsing and filtering
   - Chart rendering validation
   - Device name readability
   - Confidence score display

4. **Story 26.4:** Manual Analysis & Real-Time Updates E2E Tests
   - Manual trigger workflow
   - Progress monitoring
   - Real-time UI updates
   - MQTT notification validation

5. **Story 26.5:** Device Intelligence Features E2E Tests
   - Utilization metrics display
   - Feature suggestions workflow
   - Capability discovery validation
   - Zigbee2MQTT integration

6. **Story 26.6:** Settings & Configuration E2E Tests
   - Configuration management
   - API key validation
   - Home Assistant connection
   - Persistence verification

### Compatibility Requirements

- [x] Existing E2E tests (17 health dashboard files) remain passing
- [x] Unit tests (56/56) continue to pass
- [x] Visual tests (Puppeteer) unaffected
- [x] Backend API endpoints unchanged
- [x] Database schema unchanged
- [x] Frontend components backward compatible

### Risk Mitigation

**Primary Risk:** E2E tests become flaky due to async operations and AI response times

**Mitigation:**
- Use Playwright's auto-wait for all assertions
- Mock OpenAI API responses for deterministic tests
- Implement proper timeout handling
- Use `test.slow()` for heavy operations
- Retry failed tests 2x before marking as failed
- Mock MQTT notifications for speed

**Secondary Risk:** Tests break when AI suggestions format changes

**Mitigation:**
- Use flexible assertions (contains text, not exact match)
- Mock suggestion data with stable fixtures
- Test structure, not exact content
- Version test data with backend changes

**Rollback Plan:**
- All tests are new (no modifications to existing code)
- Can disable AI automation tests via config flag
- Test files isolated in dedicated directory
- Zero impact on production code

### Definition of Done

- [x] All 6 stories completed with acceptance criteria met
- [x] Minimum 30 E2E tests implemented and passing
- [x] Tests cover all critical user workflows
- [x] Test execution time <5 minutes
- [x] Zero flaky tests in 10 consecutive runs
- [x] Error scenarios comprehensively tested
- [x] Documentation includes test examples
- [x] CI/CD integration complete
- [x] Existing health dashboard tests still passing (17 files)
- [x] Test coverage report generated

---

## Test Architecture (Playwright Best Practices from Context7)

### 1. Test Organization

```typescript
tests/e2e/
├── ai-automation-approval-workflow.spec.ts     // Story 26.1
├── ai-automation-rejection-workflow.spec.ts    // Story 26.2
├── ai-automation-patterns.spec.ts              // Story 26.3
├── ai-automation-analysis.spec.ts              // Story 26.4
├── ai-automation-device-intelligence.spec.ts   // Story 26.5
├── ai-automation-settings.spec.ts              // Story 26.6
└── page-objects/
    ├── DashboardPage.ts
    ├── PatternsPage.ts
    ├── DeployedPage.ts
    └── SettingsPage.ts
```

### 2. Page Object Model Example

```typescript
// tests/e2e/page-objects/DashboardPage.ts
import { Page, expect } from '@playwright/test';

export class DashboardPage {
  constructor(private page: Page) {}

  async goto() {
    await this.page.goto('http://localhost:3001');
  }

  async getSuggestionCards() {
    return this.page.getByTestId('suggestion-card');
  }

  async approveSuggestion(index: number) {
    const card = this.page.getByTestId('suggestion-card').nth(index);
    await card.getByRole('button', { name: 'Approve' }).click();
  }

  async deploySuggestion(id: string) {
    await this.page.getByTestId(`deploy-${id}`).click();
    await expect(this.page.getByTestId('toast-success')).toBeVisible();
  }

  async filterByCategory(category: string) {
    await this.page.getByRole('combobox', { name: 'Category' }).selectOption(category);
  }
}
```

### 3. Test Example (Story 26.1)

```typescript
// tests/e2e/ai-automation-approval-workflow.spec.ts
import { test, expect } from '@playwright/test';
import { DashboardPage } from './page-objects/DashboardPage';
import { DeployedPage } from './page-objects/DeployedPage';

test.describe('AI Automation Approval Workflow', () => {
  let dashboardPage: DashboardPage;
  let deployedPage: DeployedPage;

  test.beforeEach(async ({ page }) => {
    dashboardPage = new DashboardPage(page);
    deployedPage = new DeployedPage(page);
    await dashboardPage.goto();
  });

  test('complete approval and deployment workflow', async ({ page }) => {
    // Step 1: Verify suggestions load
    const suggestions = await dashboardPage.getSuggestionCards();
    await expect(suggestions).toHaveCount({ min: 1 });

    // Step 2: Approve first suggestion
    await dashboardPage.approveSuggestion(0);

    // Step 3: Deploy to Home Assistant
    const suggestionId = await page.getByTestId('suggestion-card').first().getAttribute('data-id');
    await dashboardPage.deploySuggestion(suggestionId!);

    // Step 4: Verify success notification
    await expect(page.getByTestId('toast-success')).toBeVisible();
    await expect(page.getByTestId('toast-success')).toContainText('Successfully deployed');

    // Step 5: Navigate to Deployed tab
    await deployedPage.goto();

    // Step 6: Verify automation appears in deployed list
    const deployedItems = await deployedPage.getDeployedAutomations();
    await expect(deployedItems).toHaveCount({ min: 1 });
    await expect(deployedItems.first()).toContainText(suggestionId!);
  });

  test('filter suggestions by category', async () => {
    // Test filtering functionality
    await dashboardPage.filterByCategory('energy');
    
    const suggestions = await dashboardPage.getSuggestionCards();
    // Verify all visible suggestions are energy category
    for (const suggestion of await suggestions.all()) {
      await expect(suggestion).toContainText('energy');
    }
  });

  test('handle deployment errors gracefully', async ({ page }) => {
    // Mock API to return error
    await page.route('**/api/deploy/*', route => route.fulfill({
      status: 500,
      body: JSON.stringify({ error: 'Home Assistant connection failed' })
    }));

    await dashboardPage.approveSuggestion(0);
    const suggestionId = await page.getByTestId('suggestion-card').first().getAttribute('data-id');
    await dashboardPage.deploySuggestion(suggestionId!);

    // Verify error handling
    await expect(page.getByTestId('toast-error')).toBeVisible();
    await expect(page.getByTestId('toast-error')).toContainText('connection failed');
  });
});
```

### 4. Test Configuration

```typescript
// tests/e2e/docker-deployment.config.ts (additions)
export default defineConfig({
  use: {
    baseURL: process.env.AI_AUTOMATION_URL || 'http://localhost:3001',
    
    // Web-first assertions (auto-wait)
    actionTimeout: 10000,
    navigationTimeout: 15000,
    
    // Screenshot on failure
    screenshot: 'only-on-failure',
    video: 'retain-on-failure',
    trace: 'retain-on-failure',
  },
  
  // Parallel execution
  fullyParallel: true,
  workers: process.env.CI ? 2 : 4,
  
  // Retry flaky tests
  retries: process.env.CI ? 2 : 1,
  
  projects: [
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] },
    },
    // Mobile testing
    {
      name: 'mobile',
      use: { ...devices['iPhone 13'] },
    },
  ],
});
```

---

## Test Coverage Breakdown

### Workflows Tested (26 scenarios minimum)

**Story 26.1: Approval Workflow (6 tests)**
1. Complete approval and deployment workflow
2. Filter suggestions by category
3. Filter by confidence level
4. Search suggestions by keyword
5. Handle deployment errors
6. Verify deployed automation in HA

**Story 26.2: Rejection Workflow (4 tests)**
1. Reject suggestion with feedback
2. Verify suggestion hidden
3. Check feedback persistence
4. Verify similar suggestions filtered

**Story 26.3: Pattern Visualization (5 tests)**
1. View time-of-day patterns
2. View co-occurrence patterns
3. Filter patterns by device
4. Chart interactions
5. Device name readability

**Story 26.4: Manual Analysis (5 tests)**
1. Trigger manual analysis
2. Monitor progress
3. Wait for completion
4. Verify new suggestions
5. MQTT notification validation

**Story 26.5: Device Intelligence (3 tests)**
1. View utilization metrics
2. See feature suggestions
3. Capability discovery status

**Story 26.6: Settings (3 tests)**
1. Update configuration
2. Validate API keys
3. Verify persistence

---

**Epic Owner:** QA Team  
**Epic Priority:** High  
**Estimated Duration:** 5-7 days  
**Dependencies:** Epic 25 (E2E Test Infrastructure), ai-automation-service running

---

**Related Epics:**
- Epic 25: E2E Test Infrastructure Enhancement (prerequisite)
- Epic AI-1: Pattern Automation (system under test)
- Epic AI-2: Device Intelligence (system under test)

**Last Updated:** October 18, 2025

