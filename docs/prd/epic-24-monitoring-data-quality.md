# Epic 24: Monitoring Data Quality & Accuracy

## Status
**Draft** - Ready for implementation

## Epic Goal
Ensure all monitoring metrics displayed to system administrators provide accurate, real-time data instead of hardcoded placeholder values, enabling effective system health assessment and troubleshooting.

## Business Value
Accurate monitoring metrics are critical for:
- **Operational Confidence:** Administrators can trust the data they see
- **Effective Troubleshooting:** Real metrics reveal actual system behavior
- **Capacity Planning:** Accurate uptime and performance data inform decisions
- **SLA Compliance:** Ability to measure and report true system availability

## Context

A comprehensive codebase audit (October 18, 2025) revealed technical debt in monitoring metrics where placeholder values mask real system behavior. While the core data pipeline (HA events → InfluxDB) is 100% accurate, some observability metrics used by administrators contain hardcoded values.

**Audit Findings:**
- Data Integrity Score: 95/100 (excellent but room for improvement)
- Core pipeline: 100% real data ✅
- Monitoring metrics: 85/100 due to 3 hardcoded placeholders ⚠️

**Impact:** Administrators cannot trust system health metrics for monitoring production systems.

**Audit Reports:**
- Executive Summary: `implementation/FAKE_DATA_AUDIT_SUMMARY.md`
- Full Analysis: `implementation/analysis/FAKE_DATA_AUDIT_REPORT.md`

## Scope

### In Scope
✅ Fix hardcoded system uptime metric (always 99.9%)  
✅ Fix hardcoded API response time (always 0ms)  
✅ Fix hardcoded active data sources list  
✅ Add proper error handling for metric calculation failures  
✅ Update documentation to reflect accurate metrics  
✅ Add tests to prevent future placeholder regressions  

### Out of Scope
❌ Changing core data pipeline (already 100% accurate)  
❌ Fixing mock data used for TypeScript types only  
❌ Modifying test simulators (legitimate development tools)  
❌ Changing fallback logic for missing service credentials  
❌ Implementing new monitoring features beyond fixing existing metrics  

## Success Criteria

1. **Zero Hardcoded Values:** No monitoring metrics return placeholder/fake data
2. **Calculation Transparency:** All metrics clearly indicate how they're calculated
3. **Error Visibility:** If metric unavailable, clearly shown as "N/A" not fake value
4. **Test Coverage:** Automated tests prevent regression to placeholder values
5. **Documentation:** Admin docs explain metric methodology and limitations

## Technical Approach

### Identified Issues

**Issue 1: System Uptime**
- **Location:** `services/data-api/src/analytics_endpoints.py:216`
- **Current:** `uptime=99.9  # TODO: Calculate from service health data`
- **Fix:** Calculate from service start timestamp
- **Complexity:** Low - Add startup timestamp tracking

**Issue 2: API Response Time**
- **Location:** `services/admin-api/src/stats_endpoints.py:488`
- **Current:** `metrics["response_time_ms"] = 0  # placeholder - not available`
- **Fix:** Implement timing middleware OR remove metric
- **Complexity:** Medium - Requires middleware OR frontend changes

**Issue 3: Active Data Sources**
- **Location:** `services/admin-api/src/stats_endpoints.py:815`
- **Current:** `return ["home_assistant", "weather_api", "sports_api"]  # Hardcoded`
- **Fix:** Query InfluxDB for measurements with recent write activity
- **Complexity:** Low - InfluxDB schema query

### Implementation Strategy

1. **Quick Wins First:** Fix uptime and data sources (low complexity)
2. **Evaluate Response Time:** Decide between implementation or removal
3. **Comprehensive Testing:** Prevent regression
4. **Documentation Update:** Remove TODO comments, add methodology

### Dependencies
- InfluxDB client (already available)
- FastAPI middleware support (already available)
- Service startup hooks (already available)

## Stories

### Story 24.1: Fix Hardcoded Monitoring Metrics ⏳ **Draft**
Implement accurate calculation for system uptime, API response time measurement (or removal), and dynamic discovery of active data sources from InfluxDB.

**Estimated Effort:** 2-3 hours  
**Complexity:** Low-Medium  
**Priority:** High (data quality issue)

## Non-Functional Requirements

### Performance
- Metric calculations must not impact API response times (< 10ms overhead)
- InfluxDB queries for data sources must be cached (5 minute TTL)
- Response time measurement (if implemented) must be lightweight

### Reliability
- Metric calculation errors must not crash endpoints
- Fallback to "N/A" or null if calculation fails
- Graceful degradation when InfluxDB unavailable

### Maintainability
- Clear code comments explaining calculation methodology
- Automated tests prevent regression to hardcoded values
- Configuration for measurement windows and cache durations

## Testing Strategy

### Unit Tests
- Uptime calculation with various start times
- Data source query result parsing
- Error handling for missing dependencies

### Integration Tests
- Verify endpoints return real data
- Confirm no hardcoded values in responses
- Test metric changes over time (uptime increases)

### Manual Testing
- Restart services, verify uptime resets to 0%
- Check InfluxDB measurements match data sources list
- Confirm dashboard displays accurate values

## Documentation Updates

### Code Documentation
- Remove all TODO comments related to fixed placeholders
- Add docstrings explaining calculation methodology
- Inline comments for complex logic

### API Documentation
- Update OpenAPI specs with accurate metric descriptions
- Document uptime calculation method (time since restart)
- Explain data source discovery logic

### User Documentation
- Add metrics explanation to admin guide
- Document difference between uptime and availability
- Clarify metric limitations and edge cases

## Risks & Mitigation

| Risk | Impact | Mitigation |
|------|--------|------------|
| Response time measurement adds overhead | Medium | Measure overhead first, remove if > 5ms |
| InfluxDB query for data sources is slow | Low | Cache results for 5 minutes |
| Service start time lost on container restart | Low | Use container start time as fallback |
| Uptime metric confusing (restart vs downtime) | Medium | Add clear documentation about calculation |

## Success Metrics

- ✅ 0 hardcoded placeholder values in monitoring endpoints
- ✅ 100% of metrics either calculated or marked "N/A"
- ✅ Test coverage > 90% for new metric calculations
- ✅ No regression found in manual testing
- ✅ Data Integrity Score improves from 95/100 to 100/100

## Timeline

**Total Epic Duration:** 1 day

| Story | Duration | Dependencies |
|-------|----------|--------------|
| 24.1 | 2-3 hours | None |

## Related Epics

- **Epic 17:** Essential Monitoring & Observability - Original monitoring implementation
- **Epic 23:** Enhanced Dependencies & Metrics - Recent metrics improvements
- **Epic 13:** Admin API Service Separation - Split admin-api and data-api

## Acceptance Criteria (Epic Level)

1. All hardcoded monitoring values replaced with calculated or removed
2. Automated tests verify metrics return real data
3. Dashboard displays accurate system health information
4. Documentation updated to reflect metric calculation methods
5. No degradation in API performance or reliability

## Notes

This epic represents "finishing what we started" - completing the monitoring system by replacing temporary placeholder values with production-ready implementations. Low risk, high value for operational confidence.

**Philosophy:** Better to show "N/A" than fake data. Transparency builds trust.

