# Story 2.1: Event Data Normalization

## Status

Draft

## Story

**As a** data analyst,  
**I want** all captured events normalized to standardized formats,  
**so that** I can perform consistent analysis across different entity types and time periods.

## Acceptance Criteria

1. All timestamps are converted to ISO 8601 UTC format regardless of source format
2. State values are standardized (e.g., "on"/"off" to boolean, numeric values to appropriate types)
3. Unit conversions are applied consistently (temperature, pressure, etc.)
4. Entity metadata is extracted and normalized (domain, device_class, friendly_name)
5. Invalid or malformed events are logged and discarded without breaking the pipeline
6. Normalization preserves original data while adding standardized fields
7. Data validation ensures only properly normalized events proceed to storage

## Tasks / Subtasks

- [ ] Task 1: Create data normalization service structure (AC: 4, 7)
  - [ ] Create `services/enrichment-pipeline/` directory structure
  - [ ] Create `services/enrichment-pipeline/src/` directory
  - [ ] Create `services/enrichment-pipeline/src/__init__.py`
  - [ ] Create `services/enrichment-pipeline/src/main.py` (service entry point)
  - [ ] Create `services/enrichment-pipeline/src/data_normalizer.py` (data normalization)
  - [ ] Create `services/enrichment-pipeline/src/influxdb_client.py` (database operations)
  - [ ] Create `services/enrichment-pipeline/requirements.txt`
  - [ ] Create `services/enrichment-pipeline/Dockerfile`

- [ ] Task 2: Implement timestamp normalization (AC: 1)
  - [ ] Create timestamp parsing and conversion utilities
  - [ ] Implement ISO 8601 UTC format conversion
  - [ ] Handle various timestamp formats from Home Assistant
  - [ ] Add timezone handling and UTC conversion
  - [ ] Implement timestamp validation and error handling

- [ ] Task 3: Implement state value standardization (AC: 2)
  - [ ] Create state value type detection system
  - [ ] Implement boolean conversion ("on"/"off" â†’ true/false)
  - [ ] Add numeric value parsing and type conversion
  - [ ] Implement string state preservation for non-standard values
  - [ ] Add state value validation and error handling

- [ ] Task 4: Implement unit conversions (AC: 3)
  - [ ] Create unit conversion system for temperature, pressure, etc.
  - [ ] Implement temperature unit conversion (Celsius, Fahrenheit, Kelvin)
  - [ ] Add pressure unit conversion (hPa, bar, psi, etc.)
  - [ ] Implement energy unit conversion (kWh, Wh, etc.)
  - [ ] Add distance and other common unit conversions

- [ ] Task 5: Implement entity metadata extraction (AC: 4)
  - [ ] Create entity ID parsing system (domain, device_class extraction)
  - [ ] Implement friendly name extraction from attributes
  - [ ] Add area/location extraction from entity attributes
  - [ ] Implement integration source detection
  - [ ] Add device metadata normalization

- [ ] Task 6: Implement comprehensive data validation (AC: 5, 7)
  - [ ] Create event validation pipeline
  - [ ] Implement required field validation
  - [ ] Add data type validation for normalized fields
  - [ ] Implement malformed event detection and logging
  - [ ] Add validation error reporting and metrics

- [ ] Task 7: Implement normalization pipeline (AC: 6)
  - [ ] Create event normalization workflow
  - [ ] Implement original data preservation
  - [ ] Add standardized field generation
  - [ ] Implement normalization error handling
  - [ ] Add normalization performance monitoring

- [ ] Task 8: Create comprehensive tests (AC: All)
  - [ ] Create `services/enrichment-pipeline/tests/` directory
  - [ ] Create `test_data_normalizer.py` for normalization testing
  - [ ] Create `test_timestamp_normalization.py` for timestamp testing
  - [ ] Create `test_state_value_standardization.py` for state testing
  - [ ] Create `test_unit_conversions.py` for unit conversion testing
  - [ ] Add integration tests for complete normalization pipeline
  - [ ] Add performance tests for normalization throughput

## Dev Notes

### Previous Story Insights
[Source: Story 1.3 completion notes]
- WebSocket event subscription and processing is established
- Event capture and basic validation is implemented
- Event processing pipeline foundation is in place
- Health monitoring with event statistics is available

### Technology Stack
[Source: architecture/tech-stack.md]

**Data Normalization Technology:**
- **Backend Language:** Python 3.11 for data processing and normalization
- **Backend Framework:** aiohttp 3.9+ for async processing pipeline
- **Database:** InfluxDB 2.7 for time-series data storage
- **Processing:** asyncio for concurrent event processing
- **Testing:** pytest 7.4+ for backend service testing

### Context7 Implementation Guidance

#### InfluxDB Client Implementation
[Source: Context7 Knowledge Base - InfluxDB]

**InfluxDB Client Class:**
```python
# services/enrichment-pipeline/src/influxdb_client.py
import asyncio
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from influxdb_client import InfluxDBClient, Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS, ASYNCHRONOUS

logger = logging.getLogger(__name__)

class InfluxDBClientManager:
    """Manages InfluxDB connections and operations"""
    
    def __init__(self, url: str, token: str, org: str, bucket: str):
        self.url = url
        self.token = token
        self.org = org
        self.bucket = bucket
        self.client: Optional[InfluxDBClient] = None
        self.write_api = None
        self.query_api = None
        self.connected = False
        
    async def connect(self) -> bool:
        """Connect to InfluxDB"""
        try:
            self.client = InfluxDBClient(
                url=self.url,
                token=self.token,
                org=self.org,
                timeout=30000  # 30 second timeout
            )
            
            # Test connection
            health = await self._check_health()
            if health:
                self.write_api = self.client.write_api(write_options=ASYNCHRONOUS)
                self.query_api = self.client.query_api()
                self.connected = True
                logger.info("Successfully connected to InfluxDB")
                return True
            else:
                logger.error("InfluxDB health check failed")
                return False
                
        except Exception as e:
            logger.error(f"Error connecting to InfluxDB: {e}")
            return False
    
    async def _check_health(self) -> bool:
        """Check InfluxDB health"""
        try:
            health = self.client.health()
            return health.status == "pass"
        except Exception as e:
            logger.error(f"InfluxDB health check error: {e}")
            return False
    
    async def write_event(self, event: Dict[str, Any]) -> bool:
        """Write a single event to InfluxDB"""
        try:
            point = self._create_point_from_event(event)
            self.write_api.write(bucket=self.bucket, org=self.org, record=point)
            return True
        except Exception as e:
            logger.error(f"Error writing event to InfluxDB: {e}")
            return False
    
    async def write_events_batch(self, events: List[Dict[str, Any]]) -> bool:
        """Write multiple events to InfluxDB in batch"""
        try:
            points = [self._create_point_from_event(event) for event in events]
            self.write_api.write(bucket=self.bucket, org=self.org, record=points)
            logger.debug(f"Successfully wrote {len(events)} events to InfluxDB")
            return True
        except Exception as e:
            logger.error(f"Error writing batch to InfluxDB: {e}")
            return False
    
    def _create_point_from_event(self, event: Dict[str, Any]) -> Point:
        """Create InfluxDB Point from normalized event"""
        try:
            # Extract basic event data
            entity_id = event.get("entity_id")
            timestamp = event.get("timestamp")
            state_value = event.get("state_value")
            
            # Create point with measurement name
            point = Point("home_assistant_events")
            
            # Set timestamp
            if timestamp:
                point.time(timestamp, WritePrecision.NS)
            
            # Add tags (for filtering and grouping)
            if entity_id:
                point.tag("entity_id", entity_id)
            
            domain = event.get("domain")
            if domain:
                point.tag("domain", domain)
            
            device_class = event.get("device_class")
            if device_class:
                point.tag("device_class", device_class)
            
            area = event.get("area")
            if area:
                point.tag("area", area)
            
            friendly_name = event.get("friendly_name")
            if friendly_name:
                point.tag("friendly_name", friendly_name)
            
            # Add fields (measurements and values)
            if state_value is not None:
                point.field("state_value", str(state_value))
                
                # Add normalized numeric value if applicable
                normalized_value = self._normalize_state_value(state_value, event.get("device_class"))
                if normalized_value is not None:
                    point.field("normalized_value", normalized_value)
            
            previous_state = event.get("previous_state")
            if previous_state is not None:
                point.field("previous_state", str(previous_state))
            
            # Add attributes as fields
            attributes = event.get("attributes", {})
            for key, value in attributes.items():
                if isinstance(value, (str, int, float, bool)):
                    point.field(f"attr_{key}", value)
            
            return point
            
        except Exception as e:
            logger.error(f"Error creating InfluxDB point: {e}")
            raise
    
    def _normalize_state_value(self, state_value: Any, device_class: Optional[str]) -> Optional[float]:
        """Normalize state value to numeric format"""
        try:
            if isinstance(state_value, (int, float)):
                return float(state_value)
            
            if isinstance(state_value, str):
                # Handle boolean states
                if state_value.lower() in ["on", "true", "1"]:
                    return 1.0
                elif state_value.lower() in ["off", "false", "0"]:
                    return 0.0
                
                # Handle numeric strings
                try:
                    return float(state_value)
                except ValueError:
                    pass
            
            return None
            
        except Exception as e:
            logger.error(f"Error normalizing state value: {e}")
            return None
    
    async def query_events(self, query: str) -> List[Dict[str, Any]]:
        """Query events from InfluxDB"""
        try:
            result = self.query_api.query(query, org=self.org)
            
            events = []
            for table in result:
                for record in table.records:
                    event = {
                        "timestamp": record.get_time().isoformat(),
                        "entity_id": record.values.get("entity_id"),
                        "domain": record.values.get("domain"),
                        "state_value": record.values.get("state_value"),
                        "normalized_value": record.values.get("normalized_value")
                    }
                    events.append(event)
            
            return events
            
        except Exception as e:
            logger.error(f"Error querying InfluxDB: {e}")
            return []
    
    async def disconnect(self):
        """Disconnect from InfluxDB"""
        if self.client:
            self.client.close()
        self.connected = False
        logger.info("Disconnected from InfluxDB")
```

#### Data Normalization Service
[Source: Context7 Knowledge Base - InfluxDB]

**Data Normalizer Implementation:**
```python
# services/enrichment-pipeline/src/data_normalizer.py
import asyncio
import logging
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
from dataclasses import dataclass
import re

logger = logging.getLogger(__name__)

@dataclass
class NormalizedEvent:
    """Normalized event structure for InfluxDB"""
    entity_id: str
    domain: str
    device_class: Optional[str]
    state_value: Any
    normalized_value: Optional[float]
    previous_state: Optional[Any]
    timestamp: str
    attributes: Dict[str, Any]
    friendly_name: Optional[str]
    area: Optional[str]
    unit_of_measurement: Optional[str]

class DataNormalizer:
    """Normalizes Home Assistant events for InfluxDB storage"""
    
    def __init__(self):
        self.unit_conversions = {
            "temperature": {
                "Â°F": {"to": "Â°C", "factor": lambda f: (f - 32) * 5/9},
                "Â°C": {"to": "Â°C", "factor": lambda c: c},
                "K": {"to": "Â°C", "factor": lambda k: k - 273.15}
            },
            "pressure": {
                "bar": {"to": "hPa", "factor": lambda b: b * 1000},
                "hPa": {"to": "hPa", "factor": lambda h: h},
                "psi": {"to": "hPa", "factor": lambda p: p * 68.9476}
            },
            "energy": {
                "Wh": {"to": "kWh", "factor": lambda w: w / 1000},
                "kWh": {"to": "kWh", "factor": lambda k: k},
                "MWh": {"to": "kWh", "factor": lambda m: m * 1000}
            }
        }
    
    async def normalize_event(self, event: Dict[str, Any]) -> Optional[NormalizedEvent]:
        """Normalize a single Home Assistant event"""
        try:
            # Extract basic event data
            entity_id = event.get("entity_id")
            if not entity_id:
                logger.warning("Event missing entity_id, skipping")
                return None
            
            # Extract domain
            domain = entity_id.split(".")[0] if "." in entity_id else "unknown"
            
            # Extract state information
            new_state = event.get("new_state", {})
            old_state = event.get("old_state", {})
            
            state_value = new_state.get("state") if new_state else None
            previous_state = old_state.get("state") if old_state else None
            
            # Extract attributes
            attributes = new_state.get("attributes", {}) if new_state else {}
            device_class = attributes.get("device_class")
            friendly_name = attributes.get("friendly_name")
            area = attributes.get("area")
            unit_of_measurement = attributes.get("unit_of_measurement")
            
            # Normalize timestamp
            timestamp = self._normalize_timestamp(event.get("time_fired"))
            
            # Normalize state value
            normalized_value = self._normalize_state_value(state_value, device_class, unit_of_measurement)
            
            # Apply unit conversions
            if unit_of_measurement and normalized_value is not None:
                converted_value, converted_unit = self._convert_units(
                    normalized_value, unit_of_measurement, device_class
                )
                if converted_value is not None:
                    normalized_value = converted_value
                    unit_of_measurement = converted_unit
            
            return NormalizedEvent(
                entity_id=entity_id,
                domain=domain,
                device_class=device_class,
                state_value=state_value,
                normalized_value=normalized_value,
                previous_state=previous_state,
                timestamp=timestamp,
                attributes=attributes,
                friendly_name=friendly_name,
                area=area,
                unit_of_measurement=unit_of_measurement
            )
            
        except Exception as e:
            logger.error(f"Error normalizing event: {e}")
            return None
    
    def _normalize_timestamp(self, timestamp: Union[str, int, float, None]) -> str:
        """Normalize timestamp to ISO 8601 UTC format"""
        try:
            if not timestamp:
                return datetime.utcnow().isoformat() + "Z"
            
            if isinstance(timestamp, (int, float)):
                # Unix timestamp
                dt = datetime.fromtimestamp(timestamp, tz=None)
                return dt.isoformat() + "Z"
            
            if isinstance(timestamp, str):
                # Parse various timestamp formats
                formats = [
                    "%Y-%m-%dT%H:%M:%S.%fZ",
                    "%Y-%m-%dT%H:%M:%SZ",
                    "%Y-%m-%dT%H:%M:%S.%f",
                    "%Y-%m-%dT%H:%M:%S",
                    "%Y-%m-%d %H:%M:%S.%f",
                    "%Y-%m-%d %H:%M:%S"
                ]
                
                for fmt in formats:
                    try:
                        dt = datetime.strptime(timestamp, fmt)
                        return dt.isoformat() + "Z"
                    except ValueError:
                        continue
                
                # If no format matches, try parsing with dateutil
                from dateutil import parser
                dt = parser.parse(timestamp)
                return dt.isoformat() + "Z"
            
            return datetime.utcnow().isoformat() + "Z"
            
        except Exception as e:
            logger.error(f"Error normalizing timestamp: {e}")
            return datetime.utcnow().isoformat() + "Z"
    
    def _normalize_state_value(self, state_value: Any, device_class: Optional[str], unit: Optional[str]) -> Optional[float]:
        """Normalize state value to numeric format"""
        try:
            if state_value is None:
                return None
            
            if isinstance(state_value, (int, float)):
                return float(state_value)
            
            if isinstance(state_value, str):
                # Handle boolean states
                if state_value.lower() in ["on", "true", "1", "open", "active"]:
                    return 1.0
                elif state_value.lower() in ["off", "false", "0", "closed", "inactive"]:
                    return 0.0
                
                # Handle numeric strings
                try:
                    return float(state_value)
                except ValueError:
                    pass
                
                # Handle percentage strings
                if state_value.endswith("%"):
                    try:
                        return float(state_value[:-1])
                    except ValueError:
                        pass
            
            return None
            
        except Exception as e:
            logger.error(f"Error normalizing state value: {e}")
            return None
    
    def _convert_units(self, value: float, unit: str, device_class: Optional[str]) -> tuple[Optional[float], Optional[str]]:
        """Convert units to standardized format"""
        try:
            if not device_class or device_class not in self.unit_conversions:
                return value, unit
            
            conversions = self.unit_conversions[device_class]
            if unit not in conversions:
                return value, unit
            
            conversion = conversions[unit]
            converted_value = conversion["factor"](value)
            converted_unit = conversion["to"]
            
            return converted_value, converted_unit
            
        except Exception as e:
            logger.error(f"Error converting units: {e}")
            return value, unit
    
    async def normalize_events_batch(self, events: List[Dict[str, Any]]) -> List[NormalizedEvent]:
        """Normalize multiple events concurrently"""
        try:
            tasks = [self.normalize_event(event) for event in events]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            normalized_events = []
            for result in results:
                if isinstance(result, NormalizedEvent):
                    normalized_events.append(result)
                elif isinstance(result, Exception):
                    logger.error(f"Error in batch normalization: {result}")
            
            return normalized_events
            
        except Exception as e:
            logger.error(f"Error in batch normalization: {e}")
            return []
```

#### Event Processing Pipeline
[Source: Context7 Knowledge Base - InfluxDB]

**Enrichment Pipeline Service:**
```python
# services/enrichment-pipeline/src/main.py
import asyncio
import logging
from aiohttp import web
from typing import Dict, Any, List
from datetime import datetime

from .data_normalizer import DataNormalizer
from .influxdb_client import InfluxDBClientManager
from .config import EnrichmentConfig

logger = logging.getLogger(__name__)

class EnrichmentPipelineService:
    """Main enrichment pipeline service"""
    
    def __init__(self, config: EnrichmentConfig):
        self.config = config
        self.normalizer = DataNormalizer()
        self.influxdb_client = InfluxDBClientManager(
            url=config.influxdb_url,
            token=config.influxdb_token,
            org=config.influxdb_org,
            bucket=config.influxdb_bucket
        )
        self.app = web.Application()
        self._setup_routes()
        self.processing_queue = asyncio.Queue(maxsize=1000)
        self.processing_tasks = []
        
    def _setup_routes(self):
        """Setup HTTP routes"""
        self.app.router.add_post('/events', self.receive_events)
        self.app.router.add_get('/health', self.health_check)
        self.app.router.add_get('/stats', self.get_stats)
    
    async def start(self):
        """Start the enrichment pipeline service"""
        try:
            # Connect to InfluxDB
            if not await self.influxdb_client.connect():
                raise Exception("Failed to connect to InfluxDB")
            
            # Start event processing tasks
            await self._start_processing_tasks()
            
            # Start HTTP server
            runner = web.AppRunner(self.app)
            await runner.setup()
            site = web.TCPSite(runner, '0.0.0.0', self.config.service_port)
            await site.start()
            
            logger.info(f"Enrichment pipeline service started on port {self.config.service_port}")
            
        except Exception as e:
            logger.error(f"Error starting enrichment pipeline: {e}")
            raise
    
    async def _start_processing_tasks(self):
        """Start event processing tasks"""
        for i in range(self.config.processing_workers):
            task = asyncio.create_task(self._process_events(f"worker-{i}"))
            self.processing_tasks.append(task)
        
        logger.info(f"Started {self.config.processing_workers} event processing workers")
    
    async def _process_events(self, worker_name: str):
        """Process events from queue"""
        batch = []
        
        while True:
            try:
                # Collect batch of events
                while len(batch) < self.config.batch_size:
                    event = await asyncio.wait_for(
                        self.processing_queue.get(),
                        timeout=1.0
                    )
                    batch.append(event)
                
                # Process batch
                if batch:
                    await self._process_batch(batch, worker_name)
                    batch.clear()
                    
            except asyncio.TimeoutError:
                # Process partial batch if timeout
                if batch:
                    await self._process_batch(batch, worker_name)
                    batch.clear()
                    
            except Exception as e:
                logger.error(f"Error in {worker_name}: {e}")
                batch.clear()
    
    async def _process_batch(self, events: List[Dict[str, Any]], worker_name: str):
        """Process a batch of events"""
        try:
            # Normalize events
            normalized_events = await self.normalizer.normalize_events_batch(events)
            
            if normalized_events:
                # Convert to InfluxDB format
                influxdb_events = [self._convert_to_influxdb_format(event) for event in normalized_events]
                
                # Write to InfluxDB
                success = await self.influxdb_client.write_events_batch(influxdb_events)
                
                if success:
                    logger.debug(f"{worker_name} processed {len(normalized_events)} events")
                else:
                    logger.error(f"{worker_name} failed to write {len(normalized_events)} events to InfluxDB")
                    
        except Exception as e:
            logger.error(f"Error processing batch in {worker_name}: {e}")
    
    def _convert_to_influxdb_format(self, event: NormalizedEvent) -> Dict[str, Any]:
        """Convert normalized event to InfluxDB format"""
        return {
            "entity_id": event.entity_id,
            "domain": event.domain,
            "device_class": event.device_class,
            "state_value": event.state_value,
            "normalized_value": event.normalized_value,
            "previous_state": event.previous_state,
            "timestamp": event.timestamp,
            "attributes": event.attributes,
            "friendly_name": event.friendly_name,
            "area": event.area,
            "unit_of_measurement": event.unit_of_measurement
        }
    
    async def receive_events(self, request: web.Request) -> web.Response:
        """Receive events from WebSocket ingestion service"""
        try:
            data = await request.json()
            
            # Handle single event or batch
            if isinstance(data, dict):
                events = [data]
            elif isinstance(data, list):
                events = data
            else:
                return web.json_response(
                    {"error": "Invalid event format"}, 
                    status=400
                )
            
            # Queue events for processing
            for event in events:
                try:
                    await asyncio.wait_for(
                        self.processing_queue.put(event),
                        timeout=self.config.queue_timeout
                    )
                except asyncio.TimeoutError:
                    logger.error("Event queue timeout - dropping event")
                    return web.json_response(
                        {"error": "Queue timeout"}, 
                        status=503
                    )
            
            return web.json_response(
                {"message": f"Queued {len(events)} events for processing"},
                status=200
            )
            
        except Exception as e:
            logger.error(f"Error receiving events: {e}")
            return web.json_response(
                {"error": "Internal server error"}, 
                status=500
            )
    
    async def health_check(self, request: web.Request) -> web.Response:
        """Health check endpoint"""
        try:
            # Check InfluxDB connection
            influxdb_healthy = await self.influxdb_client._check_health()
            
            status = "healthy" if influxdb_healthy else "unhealthy"
            
            health_data = {
                "status": status,
                "timestamp": datetime.utcnow().isoformat(),
                "service": "enrichment-pipeline",
                "influxdb": {
                    "connected": self.influxdb_client.connected,
                    "healthy": influxdb_healthy
                },
                "processing": {
                    "queue_size": self.processing_queue.qsize(),
                    "active_workers": len(self.processing_tasks)
                }
            }
            
            status_code = 200 if status == "healthy" else 503
            return web.json_response(health_data, status=status_code)
            
        except Exception as e:
            logger.error(f"Health check error: {e}")
            return web.json_response(
                {"status": "unhealthy", "error": str(e)}, 
                status=503
            )
    
    async def get_stats(self, request: web.Request) -> web.Response:
        """Get processing statistics"""
        try:
            stats = {
                "timestamp": datetime.utcnow().isoformat(),
                "queue_size": self.processing_queue.qsize(),
                "active_workers": len(self.processing_tasks),
                "influxdb_connected": self.influxdb_client.connected
            }
            
            return web.json_response(stats, status=200)
            
        except Exception as e:
            logger.error(f"Stats error: {e}")
            return web.json_response(
                {"error": str(e)}, 
                status=500
            )
```

#### Configuration Management
[Source: Context7 Knowledge Base - InfluxDB]

**Configuration Class:**
```python
# services/enrichment-pipeline/src/config.py
import os
from dataclasses import dataclass
from typing import Optional

@dataclass
class EnrichmentConfig:
    """Configuration for enrichment pipeline service"""
    
    # InfluxDB Configuration
    influxdb_url: str
    influxdb_token: str
    influxdb_org: str
    influxdb_bucket: str
    
    # Service Configuration
    service_port: int = 8081
    processing_workers: int = 3
    batch_size: int = 100
    queue_timeout: int = 30
    
    # Logging Configuration
    log_level: str = "INFO"
    log_format: str = "json"
    
    @classmethod
    def from_env(cls) -> 'EnrichmentConfig':
        """Load configuration from environment variables"""
        
        influxdb_url = os.getenv('INFLUXDB_URL')
        if not influxdb_url:
            raise ValueError("INFLUXDB_URL environment variable is required")
        
        influxdb_token = os.getenv('INFLUXDB_TOKEN')
        if not influxdb_token:
            raise ValueError("INFLUXDB_TOKEN environment variable is required")
        
        influxdb_org = os.getenv('INFLUXDB_ORG')
        if not influxdb_org:
            raise ValueError("INFLUXDB_ORG environment variable is required")
        
        influxdb_bucket = os.getenv('INFLUXDB_BUCKET')
        if not influxdb_bucket:
            raise ValueError("INFLUXDB_BUCKET environment variable is required")
        
        return cls(
            influxdb_url=influxdb_url,
            influxdb_token=influxdb_token,
            influxdb_org=influxdb_org,
            influxdb_bucket=influxdb_bucket,
            service_port=int(os.getenv('SERVICE_PORT', '8081')),
            processing_workers=int(os.getenv('PROCESSING_WORKERS', '3')),
            batch_size=int(os.getenv('BATCH_SIZE', '100')),
            queue_timeout=int(os.getenv('QUEUE_TIMEOUT', '30')),
            log_level=os.getenv('LOG_LEVEL', 'INFO'),
            log_format=os.getenv('LOG_FORMAT', 'json')
        )
```

#### Testing Data Normalization
[Source: Context7 Knowledge Base - InfluxDB]

**Data Normalization Tests:**
```python
# services/enrichment-pipeline/tests/test_data_normalizer.py
import pytest
from datetime import datetime
from services.enrichment_pipeline.src.data_normalizer import DataNormalizer

@pytest.mark.asyncio
async def test_timestamp_normalization():
    """Test timestamp normalization"""
    normalizer = DataNormalizer()
    
    # Test ISO format
    result = await normalizer.normalize_event({
        "entity_id": "sensor.temperature",
        "time_fired": "2024-12-19T15:30:00Z",
        "new_state": {"state": "22.5"}
    })
    
    assert result.timestamp == "2024-12-19T15:30:00Z"
    
    # Test Unix timestamp
    result = await normalizer.normalize_event({
        "entity_id": "sensor.temperature",
        "time_fired": 1703004600,
        "new_state": {"state": "22.5"}
    })
    
    assert result.timestamp == "2024-12-19T15:30:00Z"

@pytest.mark.asyncio
async def test_state_value_normalization():
    """Test state value normalization"""
    normalizer = DataNormalizer()
    
    # Test boolean conversion
    result = await normalizer.normalize_event({
        "entity_id": "switch.lamp",
        "time_fired": "2024-12-19T15:30:00Z",
        "new_state": {"state": "on"}
    })
    
    assert result.normalized_value == 1.0
    
    # Test numeric conversion
    result = await normalizer.normalize_event({
        "entity_id": "sensor.temperature",
        "time_fired": "2024-12-19T15:30:00Z",
        "new_state": {"state": "22.5"}
    })
    
    assert result.normalized_value == 22.5

@pytest.mark.asyncio
async def test_unit_conversion():
    """Test unit conversion"""
    normalizer = DataNormalizer()
    
    # Test temperature conversion
    result = await normalizer.normalize_event({
        "entity_id": "sensor.temperature",
        "time_fired": "2024-12-19T15:30:00Z",
        "new_state": {
            "state": "72",
            "attributes": {
                "unit_of_measurement": "Â°F",
                "device_class": "temperature"
            }
        }
    })
    
    assert result.normalized_value == pytest.approx(22.22, rel=1e-2)  # 72Â°F to Â°C
    assert result.unit_of_measurement == "Â°C"

@pytest.mark.asyncio
async def test_influxdb_client():
    """Test InfluxDB client operations"""
    from services.enrichment_pipeline.src.influxdb_client import InfluxDBClientManager
    
    # Mock InfluxDB client for testing
    client = InfluxDBClientManager(
        url="http://localhost:8086",
        token="test_token",
        org="test_org",
        bucket="test_bucket"
    )
    
    # Test point creation
    event = {
        "entity_id": "sensor.temperature",
        "domain": "sensor",
        "device_class": "temperature",
        "state_value": "22.5",
        "normalized_value": 22.5,
        "timestamp": "2024-12-19T15:30:00Z",
        "attributes": {"unit_of_measurement": "Â°C"},
        "friendly_name": "Living Room Temperature",
        "area": "living_room"
    }
    
    point = client._create_point_from_event(event)
    
    assert point.measurement == "home_assistant_events"
    assert point._tags["entity_id"] == "sensor.temperature"
    assert point._fields["state_value"] == "22.5"
    assert point._fields["normalized_value"] == 22.5
```

### Data Normalization Requirements
[Source: architecture/core-workflows.md]

**Primary Data Ingestion Workflow:**
```mermaid
sequenceDiagram
    participant HA as Home Assistant
    participant WS as WebSocket Service
    participant ENRICH as Enrichment Pipeline

    HA->>WS: state_changed event
    WS->>WS: Validate event format
    WS->>ENRICH: Send normalized event
```

**Normalization Process:**
- Raw HA events â†’ Event validation â†’ Event parsing â†’ Event normalization â†’ Forward to storage

### Data Models and Schema
[Source: architecture/data-models.md]

**HomeAssistantEvent Interface:**
```typescript
interface HomeAssistantEvent {
  entity_id: string;
  domain: string;
  device_class?: string;
  state_value: any;
  previous_state?: any;
  timestamp: string; // ISO 8601 UTC
  weather_data?: WeatherContext;
  attributes: Record<string, any>;
}
```

**Database Schema Requirements:**
[Source: architecture/database-schema.md]

**InfluxDB Schema Design:**
- **Database:** `home_assistant`
- **Organization:** `home_assistant`
- **Bucket:** `events`
- **Measurement:** `home_assistant_events`

**Tags (for filtering and grouping):**
- `entity_id`, `domain`, `device_class`, `area`, `device_name`, `integration`

**Fields (measurements and values):**
- `state_value`, `previous_state`, `normalized_value`, `confidence`, `duration_seconds`

### Normalization Examples
[Source: architecture/database-schema.md]

**Temperature Sensor Event:**
```
Tags:
  entity_id: sensor.living_room_temperature
  domain: sensor
  device_class: temperature
  area: living_room
Fields:
  state_value: "22.5"
  normalized_value: 22.5
  unit_of_measurement: "Â°C"
Timestamp: 2024-12-19T15:30:00Z
```

**Switch Event:**
```
Tags:
  entity_id: switch.living_room_lamp
  domain: switch
  device_class: switch
  area: living_room
Fields:
  state_value: "on"
  normalized_value: 1.0
  energy_consumption: 0.05
Timestamp: 2024-12-19T15:30:00Z
```

### File Locations
[Source: architecture/unified-project-structure.md]

**Enrichment Pipeline Service Structure:**
```
services/enrichment-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                # Service entry point
â”‚   â”œâ”€â”€ weather_service.py     # Weather API integration
â”‚   â”œâ”€â”€ data_normalizer.py     # Data normalization
â”‚   â””â”€â”€ influxdb_client.py     # Database operations
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_normalizer.py
â”‚   â”œâ”€â”€ test_timestamp_normalization.py
â”‚   â”œâ”€â”€ test_state_value_standardization.py
â”‚   â””â”€â”€ test_unit_conversions.py
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```

### Environment Variables
[Source: architecture/development-workflow.md]

**Required Environment Variables:**
```bash
# InfluxDB Configuration
INFLUXDB_URL=http://influxdb:8086
INFLUXDB_TOKEN=your_influxdb_token_here
INFLUXDB_ORG=home_assistant
INFLUXDB_BUCKET=events

# Data Processing Configuration
NORMALIZATION_BATCH_SIZE=100
NORMALIZATION_TIMEOUT=30
UNIT_CONVERSION_PRECISION=2

# Logging Configuration
LOG_LEVEL=INFO
LOG_FORMAT=json
```

### Testing Requirements
[Source: architecture/testing-strategy.md]

**Backend Test Organization:**
```
services/enrichment-pipeline/tests/
â”œâ”€â”€ test_data_normalizer.py
â”œâ”€â”€ test_timestamp_normalization.py
â”œâ”€â”€ test_state_value_standardization.py
â”œâ”€â”€ test_unit_conversions.py
â””â”€â”€ test_integration_normalization.py
```

**Test Examples:**
```python
import pytest
from services.enrichment_pipeline.src.data_normalizer import DataNormalizer

def test_timestamp_normalization():
    normalizer = DataNormalizer()
    
    # Test various timestamp formats
    result = normalizer.normalize_timestamp("2024-12-19 15:30:00")
    assert result == "2024-12-19T15:30:00Z"
    
    result = normalizer.normalize_timestamp(1703004600)  # Unix timestamp
    assert result == "2024-12-19T15:30:00Z"

def test_state_value_standardization():
    normalizer = DataNormalizer()
    
    # Test boolean conversion
    result = normalizer.normalize_state_value("on", "switch")
    assert result == {"state_value": "on", "normalized_value": 1.0}
    
    # Test numeric conversion
    result = normalizer.normalize_state_value("22.5", "sensor")
    assert result == {"state_value": "22.5", "normalized_value": 22.5}
```

### Coding Standards
[Source: architecture/coding-standards.md]

**Critical Rules:**
- **Data Validation:** All events must be validated before normalization
- **Error Handling:** All normalization errors must be logged with context
- **Naming Conventions:** 
  - Functions: snake_case (e.g., `normalize_timestamp()`)
  - Data Models: PascalCase (e.g., `HomeAssistantEvent`)
  - Database Fields: snake_case (e.g., `normalized_value`)

### Performance Considerations
[Source: architecture/security-and-performance.md]

**Data Processing Performance:**
- Asynchronous event processing for high throughput
- Batch processing for efficient normalization
- Memory-efficient data structures
- Caching for frequently used conversions
- Normalization timeout configuration

### Unit Conversion Standards
[Source: architecture/database-schema.md]

**Standardized Units:**
- **Temperature:** Celsius (Â°C)
- **Pressure:** hPa (hectopascal)
- **Energy:** kWh (kilowatt-hours)
- **Distance:** meters (m)
- **Time:** seconds (s)

**Conversion Factors:**
- Fahrenheit to Celsius: (F - 32) * 5/9
- Bar to hPa: bar * 1000
- Wh to kWh: Wh / 1000

### Monitoring and Observability
[Source: architecture/monitoring-and-observability.md]

**Normalization Metrics:**
- Events normalized per minute
- Normalization error rate
- Unit conversion success rate
- Processing latency per event
- Data validation success rate

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2024-12-19 | 1.0 | Initial story creation from Epic 2.1 | Scrum Master Bob |

## Dev Agent Record

*This section will be populated by the development agent during implementation*

### Agent Model Used

*To be filled by dev agent*

### Debug Log References

*To be filled by dev agent*

### Completion Notes List

*To be filled by dev agent*

### File List

*To be filled by dev agent*

## QA Results

*Results from QA Agent review will be populated here*
